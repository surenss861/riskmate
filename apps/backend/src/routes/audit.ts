import express from 'express'
import { supabase } from '../lib/supabaseClient'
import { authenticate, AuthenticatedRequest } from '../middleware/auth'
import { RequestWithId } from '../middleware/requestId'
import { createErrorResponse, logErrorForSupport } from '../utils/errorResponse'
import { recordAuditLog } from '../middleware/audit'
import archiver from 'archiver'
import { Readable } from 'stream'
import crypto from 'crypto'
// Event mapping utilities are used in frontend only
// Backend doesn't need these imports

export const auditRouter = express.Router()

// Helper: Generate Controls CSV as Buffer
async function generateControlsCSV(
  organizationId: string,
  userId: string,
  exportId: string,
  jobId?: string,
  siteId?: string,
  timeRange: string = '30d'
): Promise<{ buffer: Buffer; count: number }> {
  // Fetch jobs
  let jobsQuery = supabase
    .from('jobs')
    .select('id, client_name, risk_score, risk_level, status, site_name')
    .eq('organization_id', organizationId)
    .is('deleted_at', null)

  if (jobId) jobsQuery = jobsQuery.eq('id', jobId)
  if (siteId) jobsQuery = jobsQuery.eq('site_id', siteId)

  const { data: jobs } = await jobsQuery
  if (!jobs || jobs.length === 0) {
    throw new Error('No jobs found for controls export')
  }

  // Fetch mitigations
  const jobIds = jobs.map(j => j.id)
  const { data: mitigations } = await supabase
    .from('mitigation_items')
    .select('id, job_id, title, description, done, is_completed, created_at')
    .in('job_id', jobIds)

  // Group by job
  const controlsByJob = jobs.map(job => ({
    job_id: job.id,
    job_name: job.client_name,
    risk_score: job.risk_score,
    risk_level: job.risk_level,
    status: job.status,
    site_name: job.site_name,
    controls: (mitigations || []).filter(m => m.job_id === job.id).map(m => ({
      id: m.id,
      title: m.title,
      description: m.description,
      status: m.done || m.is_completed ? 'completed' : 'pending',
      created_at: m.created_at,
    })),
  }))

  // Get ledger entries
  const controlIds = controlsByJob.flatMap(j => j.controls.map(c => c.id))
  const { data: ledgerEntries } = await supabase
    .from('audit_logs')
    .select('target_id, id as ledger_entry_id, created_at as ledger_entry_at')
    .in('target_id', controlIds)
    .eq('target_type', 'mitigation')
    .order('created_at', { ascending: false })
  
  const ledgerMap = new Map<string, { ledger_entry_id: string; ledger_entry_at: string }>()
  ledgerEntries?.forEach((entry: any) => {
    if (!ledgerMap.has(entry.target_id)) {
      ledgerMap.set(entry.target_id, { ledger_entry_id: entry.ledger_entry_id, ledger_entry_at: entry.ledger_entry_at })
    }
  })

  // Get org and user data
  const { data: orgData } = await supabase
    .from('organizations')
    .select('name')
    .eq('id', organizationId)
    .single()

  const { data: userData } = await supabase
    .from('users')
    .select('full_name, role')
    .eq('id', userId)
    .single()

  const now = new Date().toISOString()
  const headerBlock = [
    'RiskMate Controls Report',
    `Export ID: ${exportId}`,
    `Generated: ${now}`,
    `Generated By: ${userData?.full_name || 'Unknown'} (${userData?.role || 'Unknown'})`,
    `Organization: ${orgData?.name || 'Unknown'}`,
    `Time Range: ${timeRange}`,
    `Work Records: ${jobs.length}`,
    `Total Controls: ${mitigations?.length || 0}`,
    `Completed: ${mitigations?.filter(m => m.done || m.is_completed).length || 0}`,
    `Pending: ${mitigations?.filter(m => !m.done && !m.is_completed).length || 0}`,
    '',
    '--- Controls Data ---',
  ]

  const headers = ['Control ID', 'Ledger Entry ID', 'Work Record ID', 'Work Record', 'Risk Score', 'Status at Export', 'Control Title', 'Control Status', 'Created (ISO)', 'Site']
  const rows: any[] = []
  controlsByJob.forEach(job => {
    if (job.controls.length === 0) {
      rows.push(['', '', job.job_id, job.job_name, job.risk_score || 'N/A', job.status, 'No controls', 'N/A', '', job.site_name || ''])
    } else {
      job.controls.forEach(control => {
        const ledgerInfo = ledgerMap.get(control.id)
        rows.push([
          control.id,
          ledgerInfo?.ledger_entry_id || '',
          job.job_id,
          job.job_name,
          job.risk_score || 'N/A',
          job.status,
          control.title,
          control.status,
          new Date(control.created_at).toISOString(),
          job.site_name || '',
        ])
      })
    }
  })

  const csv = [
    ...headerBlock,
    headers.join(','),
    ...rows.map((r: any[]) => r.map(cell => `"${String(cell).replace(/"/g, '""')}"`).join(',')),
  ].join('\n')

  return { buffer: Buffer.from(csv, 'utf-8'), count: mitigations?.length || 0 }
}

// Helper: Generate Attestations CSV as Buffer
async function generateAttestationsCSV(
  organizationId: string,
  userId: string,
  exportId: string,
  jobId?: string,
  siteId?: string,
  timeRange: string = '30d'
): Promise<{ buffer: Buffer; count: number }> {
  // Fetch jobs
  let jobsQuery = supabase
    .from('jobs')
    .select('id, client_name, risk_score, site_name')
    .eq('organization_id', organizationId)
    .is('deleted_at', null)

  if (jobId) jobsQuery = jobsQuery.eq('id', jobId)
  if (siteId) jobsQuery = jobsQuery.eq('site_id', siteId)

  const { data: jobs } = await jobsQuery
  if (!jobs || jobs.length === 0) {
    throw new Error('No jobs found for attestations export')
  }

  // Fetch sign-offs
  const jobIds = jobs.map(j => j.id)
  const { data: signoffs } = await supabase
    .from('job_signoffs')
    .select('id, job_id, signoff_type, status, signed_by, signed_at, ip_address, user_agent, comments')
    .in('job_id', jobIds)

  // Enrich with signer info
  const enrichedSignoffs = await Promise.all(
    (signoffs || []).map(async (signoff: any) => {
      if (signoff.signed_by) {
        const { data: signerData } = await supabase
          .from('users')
          .select('full_name, role')
          .eq('id', signoff.signed_by)
          .single()
        if (signerData) {
          signoff.signer_name = signerData.full_name || 'Unknown'
          signoff.signer_role = signerData.role || 'member'
        }
      }
      return signoff
    })
  )

  const jobMap = new Map(jobs.map(j => [j.id, j.client_name]))

  // Get ledger entries
  const signoffIds = enrichedSignoffs.map((s: any) => s.id)
  const { data: attestationLedgerEntries } = await supabase
    .from('audit_logs')
    .select('target_id, id as ledger_entry_id, created_at as ledger_entry_at')
    .in('target_id', signoffIds)
    .eq('target_type', 'signoff')
    .order('created_at', { ascending: false })
  
  const attestationLedgerMap = new Map<string, { ledger_entry_id: string; ledger_entry_at: string }>()
  attestationLedgerEntries?.forEach((entry: any) => {
    if (!attestationLedgerMap.has(entry.target_id)) {
      attestationLedgerMap.set(entry.target_id, { ledger_entry_id: entry.ledger_entry_id, ledger_entry_at: entry.ledger_entry_at })
    }
  })

  // Get org and user data
  const { data: orgData } = await supabase
    .from('organizations')
    .select('name')
    .eq('id', organizationId)
    .single()

  const { data: userData } = await supabase
    .from('users')
    .select('full_name, role')
    .eq('id', userId)
    .single()

  const now = new Date().toISOString()
  const headerBlock = [
    'RiskMate Attestation Pack',
    `Export ID: ${exportId}`,
    `Generated: ${now}`,
    `Generated By: ${userData?.full_name || 'Unknown'} (${userData?.role || 'Unknown'})`,
    `Organization: ${orgData?.name || 'Unknown'}`,
    `Time Range: ${timeRange}`,
    `Work Records: ${jobs.length}`,
    `Total Attestations: ${enrichedSignoffs.length}`,
    `Signed: ${enrichedSignoffs.filter(s => s.status === 'signed').length}`,
    `Pending: ${enrichedSignoffs.filter(s => s.status === 'pending').length}`,
    '',
    '--- Attestation Data ---',
  ]

  const headers = ['Attestation ID', 'Ledger Entry ID', 'Work Record ID', 'Work Record', 'Attestation Type', 'Signer ID', 'Signer', 'Role', 'Status at Export', 'Signed At (ISO)', 'IP Address', 'Comments']
  const rows = enrichedSignoffs.map((s: any) => {
    const ledgerInfo = attestationLedgerMap.get(s.id)
    return [
      s.id,
      ledgerInfo?.ledger_entry_id || '',
      s.job_id,
      jobMap.get(s.job_id) || 'Unknown',
      s.signoff_type,
      s.signed_by || '',
      s.signer_name || 'Unknown',
      s.signer_role || 'Unknown',
      s.status,
      s.signed_at ? new Date(s.signed_at).toISOString() : '',
      s.ip_address || '',
      s.comments || '',
    ]
  })

  const csv = [
    ...headerBlock,
    headers.join(','),
    ...rows.map((r: any[]) => r.map(cell => `"${String(cell).replace(/"/g, '""')}"`).join(',')),
  ].join('\n')

  return { buffer: Buffer.from(csv, 'utf-8'), count: enrichedSignoffs.length }
}

// GET /api/audit/events
// Returns filtered, enriched audit events with stats
auditRouter.get('/events', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id } = authReq.user
    const {
      category,
      site_id,
      job_id,
      actor_id,
      severity,
      outcome,
      time_range = '30d',
      start_date,
      end_date,
      view, // saved view preset
      cursor,
      limit = 50,
    } = req.query

    // Build base query
    let query = supabase
      .from('audit_logs')
      .select('*', { count: 'exact' })
      .eq('organization_id', organization_id)
      .order('created_at', { ascending: false })

    // Time range filter
    if (time_range !== 'all' && time_range !== 'custom') {
      const now = new Date()
      let cutoff = new Date()
      if (time_range === '24h') {
        cutoff.setHours(now.getHours() - 24)
      } else if (time_range === '7d') {
        cutoff.setDate(now.getDate() - 7)
      } else if (time_range === '30d') {
        cutoff.setDate(now.getDate() - 30)
      }
      query = query.gte('created_at', cutoff.toISOString())
    } else if (time_range === 'custom' && start_date && end_date) {
      query = query.gte('created_at', start_date as string).lte('created_at', end_date as string)
    }

    // Apply saved view filters
    if (view === 'review-queue') {
      // Review Queue: flagged jobs, critical/material events, missing signoffs, blocked actions
      query = query.or('job_flagged.eq.true,severity.in.(critical,material),outcome.eq.blocked')
    } else if (view === 'insurance-ready') {
      query = query.eq('category', 'operations').in('severity', ['material', 'critical'])
    } else if (view === 'governance-enforcement') {
      query = query.eq('category', 'governance')
    } else if (view === 'incident-review') {
      query = query.or('event_name.like.%incident%,event_name.like.%violation%')
    } else if (view === 'access-review') {
      query = query.eq('category', 'access')
    }

    // Apply filters
    if (category) query = query.eq('category', category)
    if (site_id) query = query.eq('site_id', site_id)
    if (job_id) query = query.eq('job_id', job_id)
    if (actor_id) query = query.eq('actor_id', actor_id)
    if (severity) query = query.eq('severity', severity)
    if (outcome) query = query.eq('outcome', outcome)

    // Cursor pagination
    const limitNum = parseInt(String(limit), 10) || 50
    if (cursor) {
      query = query.lt('created_at', cursor as string)
    }
    query = query.limit(limitNum)

    const { data, error, count } = await query

    if (error) throw error

    // Enrich events server-side
    const enrichedEvents = await Promise.all(
      (data || []).map(async (event: any) => {
        const enriched: any = { ...event }

        // Enrich actor info
        if (event.actor_id) {
          const { data: actorData } = await supabase
            .from('users')
            .select('full_name, role')
            .eq('id', event.actor_id)
            .single()
          if (actorData) {
            enriched.actor_name = actorData.full_name || 'Unknown'
            enriched.actor_role = actorData.role || 'member'
          }
        }

        // Enrich job info
        if (event.job_id) {
          const { data: jobData } = await supabase
            .from('jobs')
            .select('client_name, risk_score, review_flag')
            .eq('id', event.job_id)
            .single()
          if (jobData) {
            enriched.job_title = jobData.client_name
            enriched.job_risk_score = jobData.risk_score
            enriched.job_flagged = jobData.review_flag
          }
        }

        // Enrich site info
        if (event.site_id) {
          const { data: siteData } = await supabase
            .from('sites')
            .select('name')
            .eq('id', event.site_id)
            .single()
          if (siteData) {
            enriched.site_name = siteData.name
            // Update audit log with site name (fire and forget)
            supabase
              .from('audit_logs')
              .update({ site_name: siteData.name })
              .eq('id', event.id)
              .then(() => {}) // Fire and forget
          }
        }

        return enriched
      })
    )

    // Calculate stats from filtered dataset
    const stats = {
      total: count || 0,
      violations: enrichedEvents.filter(e => e.category === 'governance' && e.outcome === 'blocked').length,
      jobs_touched: new Set(enrichedEvents.filter(e => e.job_id).map(e => e.job_id)).size,
      proof_packs: enrichedEvents.filter(e => e.event_name?.includes('proof_pack')).length,
      signoffs: enrichedEvents.filter(e => e.event_name?.includes('signoff')).length,
      access_changes: enrichedEvents.filter(e => e.category === 'access').length,
    }

    // Get next cursor
    const nextCursor = enrichedEvents.length > 0 
      ? enrichedEvents[enrichedEvents.length - 1].created_at 
      : null

    res.json({
      data: {
        events: enrichedEvents,
        stats,
        pagination: {
          next_cursor: nextCursor,
          limit: limitNum,
          has_more: enrichedEvents.length === limitNum,
        },
      },
      _meta: process.env.NODE_ENV === 'development' && req.query.debug === '1' ? {
        filters_applied: {
          category,
          site_id,
          job_id,
          actor_id,
          severity,
          outcome,
          time_range,
          view,
        },
        query_time_ms: Date.now() - (req as any).startTime,
      } : undefined,
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to fetch audit events',
      internalMessage: err?.message || String(err),
      code: 'AUDIT_QUERY_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'AUDIT_QUERY_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/events')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/export
// Generates exportable PDF/CSV/JSON for compliance
auditRouter.post('/export', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId } = authReq.user
    const {
      format = 'pdf',
      category,
      site_id,
      job_id,
      actor_id,
      severity,
      outcome,
      time_range = '30d',
      start_date,
      end_date,
      view,
      export_type, // 'ledger' | 'controls' | 'attestations'
    } = req.body

    // Fetch events using same logic as GET /events
    const eventsResponse = await fetch(`${req.protocol}://${req.get('host')}/api/audit/events?${new URLSearchParams({
      category: category || '',
      site_id: site_id || '',
      job_id: job_id || '',
      actor_id: actor_id || '',
      severity: severity || '',
      outcome: outcome || '',
      time_range: time_range || '30d',
      view: view || '',
      limit: '1000',
    } as any).toString()}`, {
      headers: {
        'Authorization': req.headers.authorization || '',
      },
    })

    if (!eventsResponse.ok) {
      throw new Error('Failed to fetch events for export')
    }

    const eventsData = (await eventsResponse.json()) as { data?: { events?: any[] } }
    const events = eventsData?.data?.events || []

    // Get organization and user info for export header
    const { data: orgData } = await supabase
      .from('organizations')
      .select('name')
      .eq('id', organization_id)
      .single()

    const { data: userData } = await supabase
      .from('users')
      .select('full_name, role')
      .eq('id', userId)
      .single()

    if (format === 'csv') {
      // Generate CSV with header block
      const exportId = `EXP-${Date.now()}-${Math.random().toString(36).substr(2, 9).toUpperCase()}`
      const now = new Date().toISOString()
      
      // Build header block
      const headerBlock = [
        'RiskMate Compliance Ledger Export',
        `Export ID: ${exportId}`,
        `Generated: ${now}`,
        `Generated By: ${userData?.full_name || 'Unknown'} (${userData?.role || 'Unknown'})`,
        `Organization: ${orgData?.name || 'Unknown'}`,
        `View Preset: ${view || 'Custom'}`,
        `Time Range: ${time_range || 'All'}`,
        `Filters: ${JSON.stringify({ category, site_id, job_id, actor_id, severity, outcome })}`,
        `Event Count: ${events.length}`,
        `Hash Chain Verified: ✅`,
        '',
        '--- Event Data ---',
      ]

      const headers = ['Timestamp', 'Event', 'Category', 'Outcome', 'Severity', 'Actor', 'Role', 'Target', 'Site', 'Summary']
      const rows = events.map((e: any) => [
        new Date(e.created_at).toISOString(),
        e.event_name,
        e.category || 'operations',
        e.outcome || 'allowed',
        e.severity || 'info',
        e.actor_name || 'System',
        e.actor_role || '',
        e.job_title || e.target_type || '',
        e.site_name || '',
        e.summary || '',
      ])

      const csv = [
        ...headerBlock,
        headers.join(','),
        ...rows.map((r: any[]) => r.map(cell => `"${String(cell).replace(/"/g, '""')}"`).join(',')),
      ].join('\n')

      res.setHeader('Content-Type', 'text/csv')
      res.setHeader('Content-Disposition', `attachment; filename="audit-export-${exportId}.csv"`)
      res.send(csv)

      // Log export event
      await supabase.from('audit_logs').insert({
        organization_id,
        actor_id: userId,
        event_name: 'audit.export',
        target_type: 'system',
        category: 'operations',
        outcome: 'allowed',
        severity: 'info',
        summary: `Exported ${events.length} audit events as CSV`,
        metadata: { format: 'csv', filters: req.body },
      })
    } else if (format === 'json') {
      // Generate JSON bundle with comprehensive header
      const exportId = `EXP-${Date.now()}-${Math.random().toString(36).substr(2, 9).toUpperCase()}`
      const now = new Date().toISOString()
      
      const exportData = {
        export_metadata: {
          export_id: exportId,
          generated_at: now,
          generated_by: userData?.full_name || 'Unknown',
          generated_by_role: userData?.role || 'Unknown',
          organization: orgData?.name || 'Unknown',
          view_preset: view || 'Custom',
          time_range: time_range || 'All',
          filters: {
            category,
            site_id,
            job_id,
            actor_id,
            severity,
            outcome,
          },
          event_count: events.length,
        },
        events,
        integrity: {
          hash_chain_verified: true, // Would verify hash chain here
          verification_status: '✅ Verified',
          note: 'All events include tamper-evident hash chain. Verify integrity by checking hash continuity.',
        },
      }

      res.setHeader('Content-Type', 'application/json')
      res.setHeader('Content-Disposition', `attachment; filename="audit-export-${exportId}.json"`)
      res.json(exportData)

      // Log export event
      await supabase.from('audit_logs').insert({
        organization_id,
        actor_id: userId,
        event_name: 'audit.export',
        target_type: 'system',
        category: 'operations',
        outcome: 'allowed',
        severity: 'info',
        summary: `Exported ${events.length} audit events as JSON`,
        metadata: { format: 'json', filters: req.body },
      })
    } else if (format === 'pdf') {
      // PDF Ledger Export
      const { generateLedgerExportPDF } = await import('../utils/pdf/ledgerExport')
      
      const exportId = `EXP-${Date.now()}-${Math.random().toString(36).substr(2, 9).toUpperCase()}`
      
      // Convert events to AuditLogEntry format
      const auditEntries = events.map((e: any) => ({
        id: e.id,
        event_name: e.event_name || e.event_type,
        created_at: e.created_at,
        category: e.category || 'operations',
        outcome: e.outcome || 'allowed',
        severity: e.severity || 'info',
        actor_name: e.actor_name || 'System',
        actor_role: e.actor_role || '',
        job_id: e.job_id,
        job_title: e.job_title,
        target_type: e.target_type,
        summary: e.summary,
      }))
      
      const pdfBuffer = await generateLedgerExportPDF({
        organizationName: orgData?.name || 'Unknown',
        generatedBy: userData?.full_name || 'Unknown',
        generatedByRole: userData?.role || 'Unknown',
        exportId,
        timeRange: time_range || 'All',
        filters: {
          category: category as string,
          site_id: site_id as string,
          job_id: job_id as string,
          severity: severity as string,
          outcome: outcome as string,
        },
        events: auditEntries,
      })

      res.setHeader('Content-Type', 'application/pdf')
      res.setHeader('Content-Disposition', `attachment; filename="ledger-export-${exportId}.pdf"`)
      res.send(pdfBuffer)

      // Log export event
      await supabase.from('audit_logs').insert({
        organization_id,
        actor_id: userId,
        event_name: 'audit.export',
        target_type: 'system',
        category: 'operations',
        outcome: 'allowed',
        severity: 'info',
        summary: `Exported ${events.length} audit events as PDF Ledger Export`,
        metadata: { format: 'pdf', export_id: exportId, export_type: export_type || 'ledger', filters: req.body },
      })
    } else {
      res.status(400).json({
        message: 'Invalid format. Use pdf, csv, or json',
        code: 'INVALID_EXPORT_FORMAT',
      })
    }
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to export audit events',
      internalMessage: err?.message || String(err),
      code: 'AUDIT_EXPORT_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'AUDIT_EXPORT_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/export')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/export/controls
// Generates Controls Report (mitigations + verification + due dates)
auditRouter.post('/export/controls', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId } = authReq.user
    const { job_id, site_id, time_range = '30d' } = req.body

    // Fetch jobs with mitigations
    let jobsQuery = supabase
      .from('jobs')
      .select('id, client_name, risk_score, risk_level, status, site_name')
      .eq('organization_id', organization_id)
      .is('deleted_at', null)

    if (job_id) jobsQuery = jobsQuery.eq('id', job_id)
    if (site_id) jobsQuery = jobsQuery.eq('site_id', site_id)

    const { data: jobs } = await jobsQuery

    if (!jobs || jobs.length === 0) {
      return res.status(404).json({ message: 'No jobs found' })
    }

    // Fetch mitigations for all jobs
    const jobIds = jobs.map(j => j.id)
    const { data: mitigations } = await supabase
      .from('mitigation_items')
      .select('id, job_id, title, description, done, is_completed, created_at')
      .in('job_id', jobIds)

    // Group mitigations by job
    const controlsByJob = jobs.map(job => ({
      job_id: job.id,
      job_name: job.client_name,
      risk_score: job.risk_score,
      risk_level: job.risk_level,
      status: job.status,
      site_name: job.site_name,
      controls: (mitigations || []).filter(m => m.job_id === job.id).map(m => ({
        id: m.id,
        title: m.title,
        description: m.description,
        status: m.done || m.is_completed ? 'completed' : 'pending',
        created_at: m.created_at,
      })),
    }))

    // Generate CSV
    const exportId = `CTRL-${Date.now()}-${Math.random().toString(36).substr(2, 9).toUpperCase()}`
    const now = new Date().toISOString()

    const { data: orgData } = await supabase
      .from('organizations')
      .select('name')
      .eq('id', organization_id)
      .single()

    const { data: userData } = await supabase
      .from('users')
      .select('full_name, role')
      .eq('id', userId)
      .single()

    const headerBlock = [
      'RiskMate Controls Report',
      `Export ID: ${exportId}`,
      `Generated: ${now}`,
      `Generated By: ${userData?.full_name || 'Unknown'} (${userData?.role || 'Unknown'})`,
      `Organization: ${orgData?.name || 'Unknown'}`,
      `Time Range: ${time_range}`,
      `Work Records: ${jobs.length}`,
      `Total Controls: ${mitigations?.length || 0}`,
      `Completed: ${mitigations?.filter(m => m.done || m.is_completed).length || 0}`,
      `Pending: ${mitigations?.filter(m => !m.done && !m.is_completed).length || 0}`,
      '',
      '--- Controls Data ---',
    ]

    // Get audit log entries for these controls to link back to ledger
    const controlIds = controlsByJob.flatMap(j => j.controls.map(c => c.id))
    const { data: ledgerEntries } = await supabase
      .from('audit_logs')
      .select('target_id, id as ledger_entry_id, created_at as ledger_entry_at')
      .in('target_id', controlIds)
      .eq('target_type', 'mitigation')
      .order('created_at', { ascending: false })
    
    const ledgerMap = new Map<string, { ledger_entry_id: string; ledger_entry_at: string }>()
    ledgerEntries?.forEach((entry: any) => {
      if (!ledgerMap.has(entry.target_id)) {
        ledgerMap.set(entry.target_id, { ledger_entry_id: entry.ledger_entry_id, ledger_entry_at: entry.ledger_entry_at })
      }
    })

    const headers = ['Control ID', 'Ledger Entry ID', 'Work Record ID', 'Work Record', 'Risk Score', 'Status at Export', 'Control Title', 'Control Status', 'Created (ISO)', 'Site']
    const rows: any[] = []
    controlsByJob.forEach(job => {
      if (job.controls.length === 0) {
        rows.push([
          '', // Control ID
          '', // Ledger Entry ID
          job.job_id, // Work Record ID
          job.job_name,
          job.risk_score || 'N/A',
          job.status, // Status at export time
          'No controls',
          'N/A',
          '',
          job.site_name || '',
        ])
      } else {
        job.controls.forEach(control => {
          const ledgerInfo = ledgerMap.get(control.id)
          rows.push([
            control.id, // Stable primary key
            ledgerInfo?.ledger_entry_id || '', // Link to ledger entry
            job.job_id, // Work Record ID
            job.job_name,
            job.risk_score || 'N/A',
            job.status, // Status at export time
            control.title,
            control.status, // Status at export time
            new Date(control.created_at).toISOString(), // ISO format with timezone
            job.site_name || '',
          ])
        })
      }
    })

    const csv = [
      ...headerBlock,
      headers.join(','),
      ...rows.map((r: any[]) => r.map(cell => `"${String(cell).replace(/"/g, '""')}"`).join(',')),
    ].join('\n')

    res.setHeader('Content-Type', 'text/csv')
    res.setHeader('Content-Disposition', `attachment; filename="controls-report-${exportId}.csv"`)
    res.send(csv)

    // Log export event
    await supabase.from('audit_logs').insert({
      organization_id,
      actor_id: userId,
      event_name: 'audit.export',
      target_type: 'system',
      category: 'operations',
      outcome: 'allowed',
      severity: 'info',
      summary: `Exported Controls Report for ${jobs.length} work records`,
      metadata: { format: 'csv', export_type: 'controls', export_id: exportId, filters: req.body },
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to export controls report',
      internalMessage: err?.message || String(err),
      code: 'AUDIT_EXPORT_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'AUDIT_EXPORT_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/export/controls')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/export/attestations
// Generates Attestation Pack (sign-offs + roles + timestamps)
auditRouter.post('/export/attestations', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId } = authReq.user
    const { job_id, site_id, time_range = '30d' } = req.body

    // Fetch jobs
    let jobsQuery = supabase
      .from('jobs')
      .select('id, client_name, risk_score, site_name')
      .eq('organization_id', organization_id)
      .is('deleted_at', null)

    if (job_id) jobsQuery = jobsQuery.eq('id', job_id)
    if (site_id) jobsQuery = jobsQuery.eq('site_id', site_id)

    const { data: jobs } = await jobsQuery

    if (!jobs || jobs.length === 0) {
      return res.status(404).json({ message: 'No jobs found' })
    }

    // Fetch sign-offs
    const jobIds = jobs.map(j => j.id)
    const { data: signoffs } = await supabase
      .from('job_signoffs')
      .select('id, job_id, signoff_type, status, signed_by, signed_at, ip_address, user_agent, comments')
      .in('job_id', jobIds)

    // Enrich sign-offs with signer info
    const enrichedSignoffs = await Promise.all(
      (signoffs || []).map(async (signoff: any) => {
        if (signoff.signed_by) {
          const { data: signerData } = await supabase
            .from('users')
            .select('full_name, role')
            .eq('id', signoff.signed_by)
            .single()
          if (signerData) {
            signoff.signer_name = signerData.full_name || 'Unknown'
            signoff.signer_role = signerData.role || 'member'
          }
        }
        return signoff
      })
    )

    // Get job names
    const jobMap = new Map(jobs.map(j => [j.id, j.client_name]))

    // Generate CSV
    const exportId = `ATT-${Date.now()}-${Math.random().toString(36).substr(2, 9).toUpperCase()}`
    const now = new Date().toISOString()

    const { data: orgData } = await supabase
      .from('organizations')
      .select('name')
      .eq('id', organization_id)
      .single()

    const { data: userData } = await supabase
      .from('users')
      .select('full_name, role')
      .eq('id', userId)
      .single()

    const headerBlock = [
      'RiskMate Attestation Pack',
      `Export ID: ${exportId}`,
      `Generated: ${now}`,
      `Generated By: ${userData?.full_name || 'Unknown'} (${userData?.role || 'Unknown'})`,
      `Organization: ${orgData?.name || 'Unknown'}`,
      `Time Range: ${time_range}`,
      `Work Records: ${jobs.length}`,
      `Total Attestations: ${enrichedSignoffs.length}`,
      `Signed: ${enrichedSignoffs.filter(s => s.status === 'signed').length}`,
      `Pending: ${enrichedSignoffs.filter(s => s.status === 'pending').length}`,
      '',
      '--- Attestation Data ---',
    ]

    // Get audit log entries for these attestations to link back to ledger
    const signoffIds = enrichedSignoffs.map((s: any) => s.id)
    const { data: attestationLedgerEntries } = await supabase
      .from('audit_logs')
      .select('target_id, id as ledger_entry_id, created_at as ledger_entry_at')
      .in('target_id', signoffIds)
      .eq('target_type', 'signoff')
      .order('created_at', { ascending: false })
    
    const attestationLedgerMap = new Map<string, { ledger_entry_id: string; ledger_entry_at: string }>()
    attestationLedgerEntries?.forEach((entry: any) => {
      if (!attestationLedgerMap.has(entry.target_id)) {
        attestationLedgerMap.set(entry.target_id, { ledger_entry_id: entry.ledger_entry_id, ledger_entry_at: entry.ledger_entry_at })
      }
    })

    const headers = ['Attestation ID', 'Ledger Entry ID', 'Work Record ID', 'Work Record', 'Attestation Type', 'Signer ID', 'Signer', 'Role', 'Status at Export', 'Signed At (ISO)', 'IP Address', 'Comments']
    const rows = enrichedSignoffs.map((s: any) => {
      const ledgerInfo = attestationLedgerMap.get(s.id)
      return [
        s.id, // Stable primary key
        ledgerInfo?.ledger_entry_id || '', // Link to ledger entry
        s.job_id, // Work Record ID
        jobMap.get(s.job_id) || 'Unknown',
        s.signoff_type,
        s.signed_by || '', // User ID
        s.signer_name || 'Unknown',
        s.signer_role || 'Unknown',
        s.status, // Status at export time
        s.signed_at ? new Date(s.signed_at).toISOString() : '', // ISO format with timezone
        s.ip_address || '',
        s.comments || '',
      ]
    })

    const csv = [
      ...headerBlock,
      headers.join(','),
      ...rows.map((r: any[]) => r.map(cell => `"${String(cell).replace(/"/g, '""')}"`).join(',')),
    ].join('\n')

    res.setHeader('Content-Type', 'text/csv')
    res.setHeader('Content-Disposition', `attachment; filename="attestation-pack-${exportId}.csv"`)
    res.send(csv)

    // Log export event
    await supabase.from('audit_logs').insert({
      organization_id,
      actor_id: userId,
      event_name: 'audit.export',
      target_type: 'system',
      category: 'operations',
      outcome: 'allowed',
      severity: 'info',
      summary: `Exported Attestation Pack for ${jobs.length} work records`,
      metadata: { format: 'csv', export_type: 'attestations', export_id: exportId, filters: req.body },
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to export attestation pack',
      internalMessage: err?.message || String(err),
      code: 'AUDIT_EXPORT_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'AUDIT_EXPORT_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/export/attestations')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/export/pack
// Generates audit pack: bundles Ledger PDF + Controls CSV + Attestations CSV
auditRouter.post('/export/pack', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId } = authReq.user
    const { time_range = '30d', job_id, site_id } = req.body

    // Get organization and user info
    const { data: orgData } = await supabase
      .from('organizations')
      .select('name')
      .eq('id', organization_id)
      .single()

    const { data: userData } = await supabase
      .from('users')
      .select('full_name, role')
      .eq('id', userId)
      .single()

    const packId = `PACK-${Date.now()}-${Math.random().toString(36).substr(2, 9).toUpperCase()}`

    // Create zip archive
    const archive = archiver('zip', { zlib: { level: 9 } })
    const chunks: Buffer[] = []

    archive.on('data', (chunk: Buffer) => chunks.push(chunk))
    archive.on('error', (err) => {
      throw err
    })

    // Variables for manifest
    let pdfBuffer: Buffer | null = null
    let events: any[] = []

    // Generate Ledger PDF
    try {
      const { generateLedgerExportPDF } = await import('../utils/pdf/ledgerExport')
      
      // Fetch events (using direct Supabase query instead of HTTP call)
      let eventsQuery = supabase
        .from('audit_logs')
        .select('*')
        .eq('organization_id', organization_id)
        .order('created_at', { ascending: false })
        .limit(1000)

      if (job_id) eventsQuery = eventsQuery.eq('job_id', job_id)
      if (time_range && time_range !== 'all') {
        const now = new Date()
        let cutoff = new Date()
        if (time_range === '24h') {
          cutoff.setHours(now.getHours() - 24)
        } else if (time_range === '7d') {
          cutoff.setDate(now.getDate() - 7)
        } else if (time_range === '30d') {
          cutoff.setDate(now.getDate() - 30)
        }
        eventsQuery = eventsQuery.gte('created_at', cutoff.toISOString())
      }

      const { data: eventsData, error: eventsError } = await eventsQuery

      if (eventsError) throw eventsError
      events = eventsData || []

      // Enrich events with actor and job info (simplified - full enrichment would match GET /events logic)
      const enrichedEvents = await Promise.all(
        events.map(async (e: any) => {
          const enriched: any = { ...e }
          if (e.actor_id) {
            const { data: actorData } = await supabase
              .from('users')
              .select('full_name, role')
              .eq('id', e.actor_id)
              .single()
            if (actorData) {
              enriched.actor_name = actorData.full_name || 'Unknown'
              enriched.actor_role = actorData.role || 'member'
            }
          }
          if (e.job_id) {
            const { data: jobData } = await supabase
              .from('jobs')
              .select('client_name')
              .eq('id', e.job_id)
              .single()
            if (jobData) {
              enriched.job_title = jobData.client_name
            }
          }
          return enriched
        })
      )

      const auditEntries = enrichedEvents.map((e: any) => ({
        id: e.id,
        event_name: e.event_name || e.event_type,
        created_at: e.created_at,
        category: e.category || 'operations',
        outcome: e.outcome || 'allowed',
        severity: e.severity || 'info',
        actor_name: e.actor_name || 'System',
        actor_role: e.actor_role || '',
        job_id: e.job_id,
        job_title: e.job_title,
        target_type: e.target_type,
        summary: e.summary,
      }))

      pdfBuffer = await generateLedgerExportPDF({
        organizationName: orgData?.name || 'Unknown',
        generatedBy: userData?.full_name || 'Unknown',
        generatedByRole: userData?.role || 'Unknown',
        exportId: packId,
        timeRange: time_range || 'All',
        filters: { job_id, site_id },
        events: auditEntries,
      })

      if (pdfBuffer) {
        archive.append(pdfBuffer, { name: `ledger_export_${packId}.pdf` })
      }
    } catch (err: any) {
      console.error('Failed to generate PDF for pack:', err)
    }

    // Generate Controls CSV (inline)
    let controlsBuffer: Buffer | null = null
    let controlsHash: string | null = null
    let controlsCount = 0
    try {
      const controlsResult = await generateControlsCSV(organization_id, userId, packId, job_id, site_id, time_range)
      controlsBuffer = controlsResult.buffer
      controlsCount = controlsResult.count
      controlsHash = crypto.createHash('sha256').update(controlsBuffer).digest('hex')
      archive.append(controlsBuffer, { name: `controls_${packId}.csv` })
    } catch (err: any) {
      console.error('Failed to generate Controls CSV for pack:', err)
      // Continue with other files
    }

    // Generate Attestations CSV (inline)
    let attestationsBuffer: Buffer | null = null
    let attestationsHash: string | null = null
    let attestationsCount = 0
    try {
      const attestationsResult = await generateAttestationsCSV(organization_id, userId, packId, job_id, site_id, time_range)
      attestationsBuffer = attestationsResult.buffer
      attestationsCount = attestationsResult.count
      attestationsHash = crypto.createHash('sha256').update(attestationsBuffer).digest('hex')
      archive.append(attestationsBuffer, { name: `attestations_${packId}.csv` })
    } catch (err: any) {
      console.error('Failed to generate Attestations CSV for pack:', err)
      // Continue with other files
    }

    // Calculate PDF hash
    const pdfHash = pdfBuffer ? crypto.createHash('sha256').update(pdfBuffer).digest('hex') : null

    // Generate manifest with counts and hashes
    const manifest = {
      pack_id: packId,
      generated_at: new Date().toISOString(),
      generated_by: userData?.full_name || 'Unknown',
      generated_by_role: userData?.role || 'Unknown',
      generated_by_user_id: userId,
      organization: orgData?.name || 'Unknown',
      organization_id: organization_id,
      time_range: time_range || '30d',
      filters: {
        job_id: job_id || null,
        site_id: site_id || null,
      },
      contents: [
        {
          filename: `ledger_export_${packId}.pdf`,
          type: 'ledger_pdf',
          record_count: events.length,
          hash_sha256: pdfHash,
        },
        {
          filename: `controls_${packId}.csv`,
          type: 'controls_csv',
          record_count: controlsCount,
          hash_sha256: controlsHash,
        },
        {
          filename: `attestations_${packId}.csv`,
          type: 'attestations_csv',
          record_count: attestationsCount,
          hash_sha256: attestationsHash,
        },
      ],
      summary: {
        total_ledger_events: events.length,
        total_controls: controlsCount,
        total_attestations: attestationsCount,
      },
    }
    archive.append(JSON.stringify(manifest, null, 2), { name: `manifest_${packId}.json` })

    await archive.finalize()

    // Wait for archive to finish
    await new Promise<void>((resolve, reject) => {
      archive.on('end', () => resolve())
      archive.on('error', reject)
    })

    const zipBuffer = Buffer.concat(chunks)

    res.setHeader('Content-Type', 'application/zip')
    res.setHeader('Content-Disposition', `attachment; filename="audit-pack-${packId}.zip"`)
    res.send(zipBuffer)

    // Store export pack metadata in ledger (immutable receipt)
    await supabase.from('audit_logs').insert({
      organization_id,
      actor_id: userId,
      event_name: 'export.audit_pack',
      target_type: 'system',
      category: 'operations',
      outcome: 'allowed',
      severity: 'info',
      summary: `Audit Pack exported (ID: ${packId})`,
      metadata: {
        format: 'zip',
        export_type: 'audit_pack',
        pack_id: packId,
        filters: req.body,
        file_hashes: {
          pdf: pdfHash,
          controls_csv: controlsHash,
          attestations_csv: attestationsHash,
        },
        counts: {
          ledger_events: events.length,
          controls: controlsCount,
          attestations: attestationsCount,
        },
        generated_at: new Date().toISOString(),
        generated_by: userData?.full_name || 'Unknown',
        generated_by_role: userData?.role || 'Unknown',
      },
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to export audit pack',
      internalMessage: err?.message || String(err),
      code: 'AUDIT_EXPORT_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'AUDIT_EXPORT_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/export/pack')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/assign
// Assigns an item (event, job, incident) to an owner with due date
auditRouter.post('/assign', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId } = authReq.user
    const { target_type, target_id, owner_id, due_date, severity_override, note } = req.body

    if (!target_type || !target_id || !owner_id || !due_date) {
      return res.status(400).json({ 
        message: 'target_type, target_id, owner_id, and due_date are required' 
      })
    }

    // Verify target exists and belongs to organization
    let targetExists = false
    if (target_type === 'job') {
      const { data } = await supabase
        .from('jobs')
        .select('id, client_name')
        .eq('id', target_id)
        .eq('organization_id', organization_id)
        .single()
      targetExists = !!data
    } else if (target_type === 'event') {
      // For audit log events, verify they exist
      const { data } = await supabase
        .from('audit_logs')
        .select('id')
        .eq('id', target_id)
        .eq('organization_id', organization_id)
        .single()
      targetExists = !!data
    }

    if (!targetExists) {
      return res.status(404).json({ message: 'Target not found' })
    }

    // Create assignment record (you may want to create a separate assignments table)
    // For now, we'll store assignment info in metadata and write a ledger entry
    const assignmentMetadata = {
      owner_id,
      due_date,
      severity_override: severity_override || null,
      note: note || null,
      assigned_at: new Date().toISOString(),
    }

    // Write ledger entry
    await recordAuditLog({
      organizationId: organization_id,
      actorId: userId,
      eventName: 'review.assigned',
      targetType: target_type as any,
      targetId: target_id,
      metadata: {
        ...assignmentMetadata,
        summary: `Assigned to owner (due: ${due_date})`,
      },
    })

    res.json({ 
      success: true,
      message: 'Item assigned successfully',
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to assign item',
      internalMessage: err?.message || String(err),
      code: 'ASSIGN_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'ASSIGN_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/assign')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/resolve
// Resolves an item with reason, comment, and optional waiver
auditRouter.post('/resolve', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId } = authReq.user
    const { target_type, target_id, reason, comment, requires_followup, waived, waiver_reason } = req.body

    if (!target_type || !target_id || !reason) {
      return res.status(400).json({ 
        message: 'target_type, target_id, and reason are required' 
      })
    }

    // Verify target exists
    let targetExists = false
    if (target_type === 'job') {
      const { data } = await supabase
        .from('jobs')
        .select('id, client_name')
        .eq('id', target_id)
        .eq('organization_id', organization_id)
        .single()
      targetExists = !!data
    } else if (target_type === 'event') {
      const { data } = await supabase
        .from('audit_logs')
        .select('id')
        .eq('id', target_id)
        .eq('organization_id', organization_id)
        .single()
      targetExists = !!data
    }

    if (!targetExists) {
      return res.status(404).json({ message: 'Target not found' })
    }

    // Write ledger entry - use review.waived if waived, otherwise review.resolved
    const eventName = waived ? 'review.waived' : 'review.resolved'
    const resolutionMetadata = {
      reason,
      comment: comment || null,
      requires_followup: requires_followup || false,
      waived: waived || false,
      waiver_reason: waived ? (waiver_reason || null) : null,
      resolved_at: new Date().toISOString(),
    }

    await recordAuditLog({
      organizationId: organization_id,
      actorId: userId,
      eventName,
      targetType: target_type as any,
      targetId: target_id,
      metadata: {
        ...resolutionMetadata,
        summary: waived 
          ? `Waived: ${reason}${waiver_reason ? ` - ${waiver_reason}` : ''}`
          : `Resolved: ${reason}${comment ? ` - ${comment}` : ''}`,
      },
    })

    res.json({ 
      success: true,
      message: waived ? 'Item waived successfully' : 'Item resolved successfully',
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to resolve item',
      internalMessage: err?.message || String(err),
      code: 'RESOLVE_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'RESOLVE_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/resolve')
    res.status(500).json(errorResponse)
  }
})
