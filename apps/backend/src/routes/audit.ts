import express, { type Router as ExpressRouter } from 'express'
import { supabase } from '../lib/supabaseClient'
import { authenticate, AuthenticatedRequest } from '../middleware/auth'
import { RequestWithId } from '../middleware/requestId'
import { createErrorResponse, logErrorForSupport } from '../utils/errorResponse'
import { recordAuditLog } from '../middleware/audit'
import { runCommand, CommandContext, CommandOptions, applyAuditFilters, FilterValidationError } from '../utils/commandRunner'
import { mapCategoryToTab, CategoryTab, eventBelongsToTab } from '../utils/categoryMapper'
import { checkIdempotency, storeIdempotencyKey, getIdempotencyKey } from '../utils/idempotency'
import archiver from 'archiver'
import { Readable } from 'stream'
import crypto from 'crypto'
// Event mapping utilities are used in frontend only
// Backend doesn't need these imports

export const auditRouter: ExpressRouter = express.Router()

// Helper: Generate Controls CSV as Buffer
async function generateControlsCSV(
  organizationId: string,
  userId: string,
  exportId: string,
  jobId?: string,
  siteId?: string,
  timeRange: string = '30d'
): Promise<{ buffer: Buffer; count: number }> {
  // Fetch jobs
  let jobsQuery = supabase
    .from('jobs')
    .select('id, client_name, risk_score, risk_level, status, site_name')
    .eq('organization_id', organizationId)
    .is('deleted_at', null)

  if (jobId) jobsQuery = jobsQuery.eq('id', jobId)
  if (siteId) jobsQuery = jobsQuery.eq('site_id', siteId)

  const { data: jobs } = await jobsQuery
  if (!jobs || jobs.length === 0) {
    throw new Error('No jobs found for controls export')
  }

  // Fetch mitigations
  const jobIds = jobs.map(j => j.id)
  const { data: mitigations } = await supabase
    .from('mitigation_items')
    .select('id, job_id, title, description, done, is_completed, created_at')
    .in('job_id', jobIds)

  // Group by job
  const controlsByJob = jobs.map(job => ({
    job_id: job.id,
    job_name: job.client_name,
    risk_score: job.risk_score,
    risk_level: job.risk_level,
    status: job.status,
    site_name: job.site_name,
    controls: (mitigations || []).filter(m => m.job_id === job.id).map(m => ({
      id: m.id,
      title: m.title,
      description: m.description,
      status: m.done || m.is_completed ? 'completed' : 'pending',
      created_at: m.created_at,
    })),
  }))

  // Note: Ledger entries are now fetched later with event_type

  // Get org and user data
  const { data: orgData } = await supabase
    .from('organizations')
    .select('name')
    .eq('id', organizationId)
    .single()

  const { data: userData } = await supabase
    .from('users')
    .select('full_name, role')
    .eq('id', userId)
    .single()

  const now = new Date().toISOString()
  const headerBlock = [
    'RiskMate Controls Report',
    `Export ID: ${exportId}`,
    `Generated: ${now}`,
    `Generated By: ${userData?.full_name || 'Unknown'} (${userData?.role || 'Unknown'})`,
    `Organization: ${orgData?.name || 'Unknown'}`,
    `Time Range: ${timeRange}`,
    `Work Records: ${jobs.length}`,
    `Total Controls: ${mitigations?.length || 0}`,
    `Completed: ${mitigations?.filter(m => m.done || m.is_completed).length || 0}`,
    `Pending: ${mitigations?.filter(m => !m.done && !m.is_completed).length || 0}`,
    '',
    '--- Controls Data ---',
  ]

  // Exact header order per schema specification
  const HEADERS = [
    'control_id',
    'ledger_entry_id',
    'ledger_event_type',
    'work_record_id',
    'site_id',
    'org_id',
    'status_at_export',
    'severity',
    'title',
    'owner_user_id',
    'owner_email',
    'due_date',
    'verification_method',
    'created_at',
    'updated_at',
  ]

  // Fetch additional data needed for schema
  const allControlIds = controlsByJob.flatMap(j => j.controls.map(c => c.id))
  const { data: controlsWithDetails } = await supabase
    .from('mitigation_items')
    .select('id, owner_id, due_date, verification_method, created_at, updated_at')
    .in('id', allControlIds)

  const controlDetailsMap = new Map(
    (controlsWithDetails || []).map(c => [c.id, c])
  )

  // Fetch owner emails
  const ownerIds = [...new Set((controlsWithDetails || []).map(c => c.owner_id).filter(Boolean))]
  const { data: owners } = await supabase
    .from('users')
    .select('id, email')
    .in('id', ownerIds)

  const ownerMap = new Map((owners || []).map(o => [o.id, o.email]))

  // Ledger entries already fetched above

  // Fetch ledger entries with event_type
  const { data: ledgerEntriesWithType } = await supabase
    .from('audit_logs')
    .select('target_id, id as ledger_entry_id, event_name as ledger_event_type')
    .in('target_id', allControlIds)
    .eq('target_type', 'mitigation')
    .order('created_at', { ascending: false })

  const ledgerTypeMap = new Map<string, { ledger_entry_id: string; ledger_event_type: string }>()
  ledgerEntriesWithType?.forEach((entry: any) => {
    if (!ledgerTypeMap.has(entry.target_id)) {
      ledgerTypeMap.set(entry.target_id, {
        ledger_entry_id: entry.ledger_entry_id,
        ledger_event_type: entry.ledger_event_type,
      })
    }
  })

  // Fetch site IDs
  const { data: jobsWithSites } = await supabase
    .from('jobs')
    .select('id, site_id')
    .in('id', jobs.map(j => j.id))

  const siteMap = new Map((jobsWithSites || []).map(j => [j.id, j.site_id || '']))

  const rows: any[] = []
  controlsByJob.forEach(job => {
    job.controls.forEach(control => {
      const ledgerInfo = ledgerTypeMap.get(control.id)
      const details = controlDetailsMap.get(control.id)
      const ownerEmail = details?.owner_id ? ownerMap.get(details.owner_id) : ''
      const siteId = siteMap.get(job.job_id) || ''

      rows.push([
        control.id, // control_id
        ledgerInfo?.ledger_entry_id || '', // ledger_entry_id
        ledgerInfo?.ledger_event_type || '', // ledger_event_type
        job.job_id, // work_record_id
        siteId, // site_id
        organizationId, // org_id
        control.status, // status_at_export
        job.risk_level || 'info', // severity
        control.title || '', // title
        details?.owner_id || '', // owner_user_id
        ownerEmail || '', // owner_email
        details?.due_date ? new Date(details.due_date).toISOString().split('T')[0] : '', // due_date (YYYY-MM-DD)
        details?.verification_method || '', // verification_method
        details?.created_at ? new Date(details.created_at).toISOString() : '', // created_at (ISO)
        details?.updated_at ? new Date(details.updated_at).toISOString() : '', // updated_at (ISO)
      ])
    })
  })

  const csv = [
    ...headerBlock,
    HEADERS.join(','),
    ...rows.map((r: any[]) => r.map(cell => `"${String(cell).replace(/"/g, '""')}"`).join(',')),
  ].join('\n')

  return { buffer: Buffer.from(csv, 'utf-8'), count: mitigations?.length || 0 }
}

// Helper: Generate Attestations CSV as Buffer
async function generateAttestationsCSV(
  organizationId: string,
  userId: string,
  exportId: string,
  jobId?: string,
  siteId?: string,
  timeRange: string = '30d'
): Promise<{ buffer: Buffer; count: number }> {
  // Fetch jobs
  let jobsQuery = supabase
    .from('jobs')
    .select('id, client_name, risk_score, site_name')
    .eq('organization_id', organizationId)
    .is('deleted_at', null)

  if (jobId) jobsQuery = jobsQuery.eq('id', jobId)
  if (siteId) jobsQuery = jobsQuery.eq('site_id', siteId)

  const { data: jobs } = await jobsQuery
  if (!jobs || jobs.length === 0) {
    throw new Error('No jobs found for attestations export')
  }

  // Fetch sign-offs
  const jobIds = jobs.map(j => j.id)
  const { data: signoffs } = await supabase
    .from('job_signoffs')
    .select('id, job_id, signoff_type, status, signed_by, signed_at, ip_address, user_agent, comments')
    .in('job_id', jobIds)

  // Enrich with signer info
  const enrichedSignoffs = await Promise.all(
    (signoffs || []).map(async (signoff: any) => {
      if (signoff.signed_by) {
        const { data: signerData } = await supabase
          .from('users')
          .select('full_name, role')
          .eq('id', signoff.signed_by)
          .single()
        if (signerData) {
          signoff.signer_name = signerData.full_name || 'Unknown'
          signoff.signer_role = signerData.role || 'member'
        }
      }
      return signoff
    })
  )

  const jobMap = new Map(jobs.map(j => [j.id, j.client_name]))

  // Get ledger entries
  const signoffIds = enrichedSignoffs.map((s: any) => s.id)
  const { data: attestationLedgerEntries } = await supabase
    .from('audit_logs')
    .select('target_id, id as ledger_entry_id, created_at as ledger_entry_at')
    .in('target_id', signoffIds)
    .eq('target_type', 'signoff')
    .order('created_at', { ascending: false })
  
  const attestationLedgerMap = new Map<string, { ledger_entry_id: string; ledger_entry_at: string }>()
  attestationLedgerEntries?.forEach((entry: any) => {
    if (!attestationLedgerMap.has(entry.target_id)) {
      attestationLedgerMap.set(entry.target_id, { ledger_entry_id: entry.ledger_entry_id, ledger_entry_at: entry.ledger_entry_at })
    }
  })

  // Get org and user data
  const { data: orgData } = await supabase
    .from('organizations')
    .select('name')
    .eq('id', organizationId)
    .single()

  const { data: userData } = await supabase
    .from('users')
    .select('full_name, role')
    .eq('id', userId)
    .single()

  const now = new Date().toISOString()
  const headerBlock = [
    'RiskMate Attestation Pack',
    `Export ID: ${exportId}`,
    `Generated: ${now}`,
    `Generated By: ${userData?.full_name || 'Unknown'} (${userData?.role || 'Unknown'})`,
    `Organization: ${orgData?.name || 'Unknown'}`,
    `Time Range: ${timeRange}`,
    `Work Records: ${jobs.length}`,
    `Total Attestations: ${enrichedSignoffs.length}`,
    `Signed: ${enrichedSignoffs.filter(s => s.status === 'signed').length}`,
    `Pending: ${enrichedSignoffs.filter(s => s.status === 'pending').length}`,
    '',
    '--- Attestation Data ---',
  ]

  // Exact header order per schema specification
  const HEADERS = [
    'attestation_id',
    'ledger_entry_id',
    'ledger_event_type',
    'work_record_id',
    'site_id',
    'org_id',
    'status_at_export',
    'signer_user_id',
    'signer_email',
    'signer_role',
    'signed_at',
    'statement',
  ]

  // Fetch site IDs and enrich signoffs with required fields
  const { data: jobsWithSites } = await supabase
    .from('jobs')
    .select('id, site_id')
    .in('id', jobIds)

  const siteMap = new Map((jobsWithSites || []).map(j => [j.id, j.site_id || '']))

  // Fetch signer emails
  const signerIds = [...new Set(enrichedSignoffs.map(s => s.signed_by).filter(Boolean))]
  const { data: signers } = await supabase
    .from('users')
    .select('id, email')
    .in('id', signerIds)

  const signerEmailMap = new Map((signers || []).map(s => [s.id, s.email]))

  // Fetch ledger entries with event_type
  const { data: attestationLedgerEntriesWithType } = await supabase
    .from('audit_logs')
    .select('target_id, id as ledger_entry_id, event_name as ledger_event_type')
    .in('target_id', signoffIds)
    .eq('target_type', 'system') // Updated from 'signoff' to match our recordAuditLog
    .order('created_at', { ascending: false })

  const attestationLedgerTypeMap = new Map<string, { ledger_entry_id: string; ledger_event_type: string }>()
  attestationLedgerEntriesWithType?.forEach((entry: any) => {
    if (!attestationLedgerTypeMap.has(entry.target_id)) {
      attestationLedgerTypeMap.set(entry.target_id, {
        ledger_entry_id: entry.ledger_entry_id,
        ledger_event_type: entry.ledger_event_type,
      })
    }
  })

  const rows = enrichedSignoffs.map((s: any) => {
    const ledgerInfo = attestationLedgerTypeMap.get(s.id)
    const signerEmail = s.signed_by ? signerEmailMap.get(s.signed_by) : ''
    const siteId = siteMap.get(s.job_id) || ''

    return [
      s.id, // attestation_id
      ledgerInfo?.ledger_entry_id || '', // ledger_entry_id
      ledgerInfo?.ledger_event_type || '', // ledger_event_type
      s.job_id, // work_record_id
      siteId, // site_id
      organizationId, // org_id
      s.status, // status_at_export
      s.signed_by || '', // signer_user_id
      signerEmail || '', // signer_email
      s.signer_role || 'Unknown', // signer_role
      s.signed_at ? new Date(s.signed_at).toISOString() : '', // signed_at (ISO)
      s.comments || '', // statement
    ]
  })

  const csv = [
    ...headerBlock,
    HEADERS.join(','),
    ...rows.map((r: any[]) => r.map(cell => `"${String(cell).replace(/"/g, '""')}"`).join(',')),
  ].join('\n')

  return { buffer: Buffer.from(csv, 'utf-8'), count: enrichedSignoffs.length }
}

// GET /api/audit/events
// Returns filtered, enriched audit events with stats
// Uses unified filter logic (same as exports/readiness) for consistency
auditRouter.get('/events', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id } = authReq.user
    const {
      category,
      site_id,
      job_id,
      actor_id,
      severity,
      outcome,
      time_range = '30d',
      start_date,
      end_date,
      view, // saved view preset
      cursor,
      limit = 50,
    } = req.query

    // Build base query with unified filters
    let query = supabase
      .from('audit_logs')
      .select('*', { count: 'exact' })
      .order('created_at', { ascending: false })

    // Apply unified audit filters (same logic as exports/readiness)
    // Note: Don't filter by category at DB level if it's a main tab (governance/operations/access)
    // We'll filter by category_tab in memory after enrichment to handle old events correctly
    const isMainTabCategory = category && ['governance', 'operations', 'access'].includes(category as string)
    try {
      query = applyAuditFilters(query, {
        organizationId: organization_id,
        // Only pass category to DB filter if it's NOT a main tab (let in-memory filtering handle main tabs)
        ...(category && !isMainTabCategory && { category: category as any }),
        ...(site_id && { site_id: site_id as string }),
        ...(job_id && { job_id: job_id as string }),
        ...(actor_id && { actor_id: actor_id as string }),
        ...(severity && { severity: severity as any }),
        ...(outcome && { outcome: outcome as any }),
        ...(time_range && { time_range: time_range as any }),
        ...(start_date && { start_date: start_date as string }),
        ...(end_date && { end_date: end_date as string }),
        ...(view && { view: view as any }),
      })
    } catch (filterError: any) {
      // Handle filter validation errors (400 Bad Request)
      if (filterError instanceof FilterValidationError) {
        const { response: errorResponse, errorId } = createErrorResponse({
          message: filterError.message,
          internalMessage: `Invalid filter parameter: ${filterError.field}`,
          code: 'INVALID_FILTER',
          requestId,
          statusCode: 400,
          field: filterError.field,
          allowedValues: filterError.allowedValues,
        })
        res.setHeader('X-Error-ID', errorId)
        logErrorForSupport(400, 'INVALID_FILTER', requestId, organization_id, errorResponse.message, errorResponse.internalMessage, 'system', 'info', '/api/audit/events')
        return res.status(400).json(errorResponse)
      }
      // Re-throw unexpected errors
      throw filterError
    }

    // Cursor pagination
    const limitNum = parseInt(String(limit), 10) || 50
    if (cursor) {
      query = query.lt('created_at', cursor as string)
    }
    query = query.limit(limitNum)

    const { data, error, count } = await query

    if (error) {
      // Log full error details for debugging
      console.error('[audit/events] Supabase query error:', {
        code: error.code,
        message: error.message,
        details: error.details,
        hint: error.hint,
        query_params: req.query,
        organization_id: organization_id,
        request_id: requestId,
      })
      
      // If it's an RLS recursion error, provide a more helpful message
      if (error.code === '42P17' || error.message?.includes('infinite recursion')) {
        const { response: errorResponse, errorId } = createErrorResponse({
          message: 'Database policy recursion detected. This indicates a configuration issue with row-level security policies.',
          internalMessage: `RLS recursion error: ${error.message} (code: ${error.code})`,
          code: 'RLS_RECURSION_ERROR',
          requestId,
          statusCode: 500,
          databaseError: {
            code: error.code,
            message: error.message,
            hint: error.hint,
            details: error.details,
          },
        })
        res.setHeader('X-Error-ID', errorId)
        logErrorForSupport(500, 'RLS_RECURSION_ERROR', requestId, organization_id, errorResponse.message, errorResponse.internalMessage, 'system', 'error', '/api/audit/events')
        return res.status(500).json(errorResponse)
      }
      
      // Return the actual Supabase error so we can debug
      const { response: errorResponse, errorId } = createErrorResponse({
        message: 'Failed to fetch audit events',
        internalMessage: error.message || String(error),
        code: 'QUERY_ERROR',
        requestId,
        statusCode: 500,
        source: 'supabase',
        databaseError: {
          code: error.code,
          message: error.message,
          hint: error.hint,
          details: error.details,
        },
        // Include the full error for debugging
        rawError: process.env.NODE_ENV === 'development' ? error : undefined,
      })
      res.setHeader('X-Error-ID', errorId)
      logErrorForSupport(500, 'QUERY_ERROR', requestId, organization_id, errorResponse.message, errorResponse.internalMessage, 'system', 'error', '/api/audit/events')
      return res.status(500).json(errorResponse)
    }

    // Enrich events server-side
    const enrichedEvents = await Promise.all(
      (data || []).map(async (event: any) => {
        const enriched: any = { ...event }

        // Add computed category_tab for frontend filtering (handles old categories)
        enriched.category_tab = mapCategoryToTab(event.category, event.event_name)

        // Enrich actor info
        if (event.actor_id) {
          const { data: actorData } = await supabase
            .from('users')
            .select('full_name, role')
            .eq('id', event.actor_id)
            .single()
          if (actorData) {
            enriched.actor_name = actorData.full_name || 'Unknown'
            enriched.actor_role = actorData.role || 'member'
          }
        }

        // Enrich job info
        if (event.job_id) {
          const { data: jobData } = await supabase
            .from('jobs')
            .select('client_name, risk_score, review_flag')
            .eq('id', event.job_id)
            .single()
          if (jobData) {
            enriched.job_title = jobData.client_name
            enriched.job_risk_score = jobData.risk_score
            enriched.job_flagged = jobData.review_flag
          }
        }

        // Enrich site info
        if (event.site_id) {
          const { data: siteData } = await supabase
            .from('sites')
            .select('name')
            .eq('id', event.site_id)
            .single()
          if (siteData) {
            enriched.site_name = siteData.name
            // Update audit log with site name (fire and forget)
            supabase
              .from('audit_logs')
              .update({ site_name: siteData.name })
              .eq('id', event.id)
              .then(() => {}) // Fire and forget
          }
        }

        return enriched
      })
    )

    // Filter by category_tab if category was requested (for tab filtering)
    // This ensures old events with different category values still show up in the right tab
    let finalEvents = enrichedEvents
    if (category && ['governance', 'operations', 'access'].includes(category as string)) {
      const requestedTab = category as CategoryTab
      finalEvents = enrichedEvents.filter(e => eventBelongsToTab(e.category, e.event_name, requestedTab))
    }

    // Calculate stats from filtered dataset (use finalEvents after tab filtering)
    const stats = {
      total: finalEvents.length,
      violations: finalEvents.filter(e => eventBelongsToTab(e.category, e.event_name, 'governance') && e.outcome === 'blocked').length,
      jobs_touched: new Set(finalEvents.filter(e => e.job_id).map(e => e.job_id)).size,
      proof_packs: finalEvents.filter(e => e.event_name?.includes('proof_pack')).length,
      signoffs: finalEvents.filter(e => e.event_name?.includes('signoff')).length,
      access_changes: finalEvents.filter(e => eventBelongsToTab(e.category, e.event_name, 'access')).length,
    }

    // Get next cursor
    const nextCursor = finalEvents.length > 0 
      ? finalEvents[finalEvents.length - 1].created_at 
      : null

    res.json({
      data: {
        events: finalEvents,
        stats,
        pagination: {
          next_cursor: nextCursor,
          limit: limitNum,
          has_more: enrichedEvents.length === limitNum,
        },
      },
      _meta: process.env.NODE_ENV === 'development' && req.query.debug === '1' ? {
        filters_applied: {
          category,
          site_id,
          job_id,
          actor_id,
          severity,
          outcome,
          time_range,
          view,
        },
        query_time_ms: Date.now() - (req as any).startTime,
      } : undefined,
    })
  } catch (err: any) {
    // Log the full error for debugging
    console.error('[audit/events] Unexpected error:', {
      message: err?.message,
      stack: err?.stack,
      code: err?.code,
      name: err?.name,
      query_params: req.query,
      organization_id: authReq.user?.organization_id,
      request_id: requestId,
    })
    
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to fetch audit events',
      internalMessage: err?.message || String(err),
      code: 'AUDIT_QUERY_ERROR',
      requestId,
      statusCode: 500,
      source: 'server',
      // Include error details for debugging (only in development)
      rawError: process.env.NODE_ENV === 'development' ? {
        message: err?.message,
        code: err?.code,
        name: err?.name,
        stack: err?.stack,
      } : undefined,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'AUDIT_QUERY_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internalMessage, 'system', 'error', '/api/audit/events')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/export
// Generates exportable PDF/CSV/JSON for compliance
auditRouter.post('/export', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId } = authReq.user
    const {
      format = 'pdf',
      category,
      site_id,
      job_id,
      actor_id,
      severity,
      outcome,
      time_range = '30d',
      start_date,
      end_date,
      view,
      export_type, // 'ledger' | 'controls' | 'attestations'
    } = req.body

    // Fetch events using same logic as GET /events
    const eventsResponse = await fetch(`${req.protocol}://${req.get('host')}/api/audit/events?${new URLSearchParams({
      category: category || '',
      site_id: site_id || '',
      job_id: job_id || '',
      actor_id: actor_id || '',
      severity: severity || '',
      outcome: outcome || '',
      time_range: time_range || '30d',
      view: view || '',
      limit: '1000',
    } as any).toString()}`, {
      headers: {
        'Authorization': req.headers.authorization || '',
      },
    })

    if (!eventsResponse.ok) {
      throw new Error('Failed to fetch events for export')
    }

    const eventsData = (await eventsResponse.json()) as { data?: { events?: any[] } }
    const events = eventsData?.data?.events || []

    // Get organization and user info for export header
    const { data: orgData } = await supabase
      .from('organizations')
      .select('name')
      .eq('id', organization_id)
      .single()

    const { data: userData } = await supabase
      .from('users')
      .select('full_name, role')
      .eq('id', userId)
      .single()

    if (format === 'csv') {
      // Generate CSV with header block
      const exportId = `EXP-${Date.now()}-${Math.random().toString(36).substr(2, 9).toUpperCase()}`
      const now = new Date().toISOString()
      
      // Build header block
      const headerBlock = [
        'RiskMate Compliance Ledger Export',
        `Export ID: ${exportId}`,
        `Generated: ${now}`,
        `Generated By: ${userData?.full_name || 'Unknown'} (${userData?.role || 'Unknown'})`,
        `Organization: ${orgData?.name || 'Unknown'}`,
        `View Preset: ${view || 'Custom'}`,
        `Time Range: ${time_range || 'All'}`,
        `Filters: ${JSON.stringify({ category, site_id, job_id, actor_id, severity, outcome })}`,
        `Event Count: ${events.length}`,
        `Hash Chain Verified: ✅`,
        '',
        '--- Event Data ---',
      ]

      const headers = ['Timestamp', 'Event', 'Category', 'Outcome', 'Severity', 'Actor', 'Role', 'Target', 'Site', 'Summary']
      const rows = events.map((e: any) => [
        new Date(e.created_at).toISOString(),
        e.event_name,
        e.category || 'operations',
        e.outcome || 'allowed',
        e.severity || 'info',
        e.actor_name || 'System',
        e.actor_role || '',
        e.job_title || e.target_type || '',
        e.site_name || '',
        e.summary || '',
      ])

      const csv = [
        ...headerBlock,
        headers.join(','),
        ...rows.map((r: any[]) => r.map(cell => `"${String(cell).replace(/"/g, '""')}"`).join(',')),
      ].join('\n')

      res.setHeader('Content-Type', 'text/csv')
      res.setHeader('Content-Disposition', `attachment; filename="audit-export-${exportId}.csv"`)
      res.send(csv)

      // Log export event
      await supabase.from('audit_logs').insert({
        organization_id,
        actor_id: userId,
        event_name: 'audit.export',
        target_type: 'system',
        category: 'operations',
        outcome: 'allowed',
        severity: 'info',
        summary: `Exported ${events.length} audit events as CSV`,
        metadata: { format: 'csv', filters: req.body },
      })
    } else if (format === 'json') {
      // Generate JSON bundle with comprehensive header
      const exportId = `EXP-${Date.now()}-${Math.random().toString(36).substr(2, 9).toUpperCase()}`
      const now = new Date().toISOString()
      
      const exportData = {
        export_metadata: {
          export_id: exportId,
          generated_at: now,
          generated_by: userData?.full_name || 'Unknown',
          generated_by_role: userData?.role || 'Unknown',
          organization: orgData?.name || 'Unknown',
          view_preset: view || 'Custom',
          time_range: time_range || 'All',
          filters: {
            category,
            site_id,
            job_id,
            actor_id,
            severity,
            outcome,
          },
          event_count: events.length,
        },
        events,
        integrity: {
          hash_chain_verified: true, // Would verify hash chain here
          verification_status: '✅ Verified',
          note: 'All events include tamper-evident hash chain. Verify integrity by checking hash continuity.',
        },
      }

      res.setHeader('Content-Type', 'application/json')
      res.setHeader('Content-Disposition', `attachment; filename="audit-export-${exportId}.json"`)
      res.json(exportData)

      // Log export event
      await supabase.from('audit_logs').insert({
        organization_id,
        actor_id: userId,
        event_name: 'audit.export',
        target_type: 'system',
        category: 'operations',
        outcome: 'allowed',
        severity: 'info',
        summary: `Exported ${events.length} audit events as JSON`,
        metadata: { format: 'json', filters: req.body },
      })
    } else if (format === 'pdf') {
      // PDF Ledger Export
      const { generateLedgerExportPDF } = await import('../utils/pdf/ledgerExport')
      
      const exportId = `EXP-${Date.now()}-${Math.random().toString(36).substr(2, 9).toUpperCase()}`
      
      // Convert events to AuditLogEntry format
      const auditEntries = events.map((e: any) => ({
        id: e.id,
        event_name: e.event_name || e.event_type,
        created_at: e.created_at,
        category: e.category || 'operations',
        outcome: e.outcome || 'allowed',
        severity: e.severity || 'info',
        actor_name: e.actor_name || 'System',
        actor_role: e.actor_role || '',
        job_id: e.job_id,
        job_title: e.job_title,
        target_type: e.target_type,
        summary: e.summary,
      }))
      
      const pdfBuffer = await generateLedgerExportPDF({
        organizationName: orgData?.name || 'Unknown',
        generatedBy: userData?.full_name || 'Unknown',
        generatedByRole: userData?.role || 'Unknown',
        exportId,
        timeRange: time_range || 'All',
        filters: {
          category: category as string,
          site_id: site_id as string,
          job_id: job_id as string,
          severity: severity as string,
          outcome: outcome as string,
        },
        events: auditEntries,
      })

      res.setHeader('Content-Type', 'application/pdf')
      res.setHeader('Content-Disposition', `attachment; filename="ledger-export-${exportId}.pdf"`)
      res.send(pdfBuffer)

      // Log export event
      await supabase.from('audit_logs').insert({
        organization_id,
        actor_id: userId,
        event_name: 'audit.export',
        target_type: 'system',
        category: 'operations',
        outcome: 'allowed',
        severity: 'info',
        summary: `Exported ${events.length} audit events as PDF Ledger Export`,
        metadata: { format: 'pdf', export_id: exportId, export_type: export_type || 'ledger', filters: req.body },
      })
    } else {
      res.status(400).json({
        message: 'Invalid format. Use pdf, csv, or json',
        code: 'INVALID_EXPORT_FORMAT',
      })
    }
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to export audit events',
      internalMessage: err?.message || String(err),
      code: 'AUDIT_EXPORT_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'AUDIT_EXPORT_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/export')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/export/controls
// Generates Controls Report (mitigations + verification + due dates)
auditRouter.post('/export/controls', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId } = authReq.user
    const { job_id, site_id, time_range = '30d' } = req.body

    // Fetch jobs with mitigations
    let jobsQuery = supabase
      .from('jobs')
      .select('id, client_name, risk_score, risk_level, status, site_name')
      .eq('organization_id', organization_id)
      .is('deleted_at', null)

    if (job_id) jobsQuery = jobsQuery.eq('id', job_id)
    if (site_id) jobsQuery = jobsQuery.eq('site_id', site_id)

    const { data: jobs } = await jobsQuery

    if (!jobs || jobs.length === 0) {
      return res.status(404).json({ message: 'No jobs found' })
    }

    // Fetch mitigations for all jobs
    const jobIds = jobs.map(j => j.id)
    const { data: mitigations } = await supabase
      .from('mitigation_items')
      .select('id, job_id, title, description, done, is_completed, created_at')
      .in('job_id', jobIds)

    // Group mitigations by job
    const controlsByJob = jobs.map(job => ({
      job_id: job.id,
      job_name: job.client_name,
      risk_score: job.risk_score,
      risk_level: job.risk_level,
      status: job.status,
      site_name: job.site_name,
      controls: (mitigations || []).filter(m => m.job_id === job.id).map(m => ({
        id: m.id,
        title: m.title,
        description: m.description,
        status: m.done || m.is_completed ? 'completed' : 'pending',
        created_at: m.created_at,
      })),
    }))

    // Generate CSV
    const exportId = `CTRL-${Date.now()}-${Math.random().toString(36).substr(2, 9).toUpperCase()}`
    const now = new Date().toISOString()

    const { data: orgData } = await supabase
      .from('organizations')
      .select('name')
      .eq('id', organization_id)
      .single()

    const { data: userData } = await supabase
      .from('users')
      .select('full_name, role')
      .eq('id', userId)
      .single()

    const headerBlock = [
      'RiskMate Controls Report',
      `Export ID: ${exportId}`,
      `Generated: ${now}`,
      `Generated By: ${userData?.full_name || 'Unknown'} (${userData?.role || 'Unknown'})`,
      `Organization: ${orgData?.name || 'Unknown'}`,
      `Time Range: ${time_range}`,
      `Work Records: ${jobs.length}`,
      `Total Controls: ${mitigations?.length || 0}`,
      `Completed: ${mitigations?.filter(m => m.done || m.is_completed).length || 0}`,
      `Pending: ${mitigations?.filter(m => !m.done && !m.is_completed).length || 0}`,
      '',
      '--- Controls Data ---',
    ]

    // Get audit log entries for these controls to link back to ledger
    const controlIds = controlsByJob.flatMap(j => j.controls.map(c => c.id))
    const { data: ledgerEntries } = await supabase
      .from('audit_logs')
      .select('target_id, id as ledger_entry_id, created_at as ledger_entry_at')
      .in('target_id', controlIds)
      .eq('target_type', 'mitigation')
      .order('created_at', { ascending: false })
    
    const ledgerMap = new Map<string, { ledger_entry_id: string; ledger_entry_at: string }>()
    ledgerEntries?.forEach((entry: any) => {
      if (!ledgerMap.has(entry.target_id)) {
        ledgerMap.set(entry.target_id, { ledger_entry_id: entry.ledger_entry_id, ledger_entry_at: entry.ledger_entry_at })
      }
    })

    const headers = ['Control ID', 'Ledger Entry ID', 'Work Record ID', 'Work Record', 'Risk Score', 'Status at Export', 'Control Title', 'Control Status', 'Created (ISO)', 'Site']
    const rows: any[] = []
    controlsByJob.forEach(job => {
      if (job.controls.length === 0) {
        rows.push([
          '', // Control ID
          '', // Ledger Entry ID
          job.job_id, // Work Record ID
          job.job_name,
          job.risk_score || 'N/A',
          job.status, // Status at export time
          'No controls',
          'N/A',
          '',
          job.site_name || '',
        ])
      } else {
        job.controls.forEach(control => {
          const ledgerInfo = ledgerMap.get(control.id)
          rows.push([
            control.id, // Stable primary key
            ledgerInfo?.ledger_entry_id || '', // Link to ledger entry
            job.job_id, // Work Record ID
            job.job_name,
            job.risk_score || 'N/A',
            job.status, // Status at export time
            control.title,
            control.status, // Status at export time
            new Date(control.created_at).toISOString(), // ISO format with timezone
            job.site_name || '',
          ])
        })
      }
    })

    const csv = [
      ...headerBlock,
      headers.join(','),
      ...rows.map((r: any[]) => r.map(cell => `"${String(cell).replace(/"/g, '""')}"`).join(',')),
    ].join('\n')

    res.setHeader('Content-Type', 'text/csv')
    res.setHeader('Content-Disposition', `attachment; filename="controls-report-${exportId}.csv"`)
    res.send(csv)

    // Log export event
    await supabase.from('audit_logs').insert({
      organization_id,
      actor_id: userId,
      event_name: 'audit.export',
      target_type: 'system',
      category: 'operations',
      outcome: 'allowed',
      severity: 'info',
      summary: `Exported Controls Report for ${jobs.length} work records`,
      metadata: { format: 'csv', export_type: 'controls', export_id: exportId, filters: req.body },
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to export controls report',
      internalMessage: err?.message || String(err),
      code: 'AUDIT_EXPORT_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'AUDIT_EXPORT_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/export/controls')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/export/attestations
// Generates Attestation Pack (sign-offs + roles + timestamps)
auditRouter.post('/export/attestations', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId } = authReq.user
    const { job_id, site_id, time_range = '30d' } = req.body

    // Fetch jobs
    let jobsQuery = supabase
      .from('jobs')
      .select('id, client_name, risk_score, site_name')
      .eq('organization_id', organization_id)
      .is('deleted_at', null)

    if (job_id) jobsQuery = jobsQuery.eq('id', job_id)
    if (site_id) jobsQuery = jobsQuery.eq('site_id', site_id)

    const { data: jobs } = await jobsQuery

    if (!jobs || jobs.length === 0) {
      return res.status(404).json({ message: 'No jobs found' })
    }

    // Fetch sign-offs
    const jobIds = jobs.map(j => j.id)
    const { data: signoffs } = await supabase
      .from('job_signoffs')
      .select('id, job_id, signoff_type, status, signed_by, signed_at, ip_address, user_agent, comments')
      .in('job_id', jobIds)

    // Enrich sign-offs with signer info
    const enrichedSignoffs = await Promise.all(
      (signoffs || []).map(async (signoff: any) => {
        if (signoff.signed_by) {
          const { data: signerData } = await supabase
            .from('users')
            .select('full_name, role')
            .eq('id', signoff.signed_by)
            .single()
          if (signerData) {
            signoff.signer_name = signerData.full_name || 'Unknown'
            signoff.signer_role = signerData.role || 'member'
          }
        }
        return signoff
      })
    )

    // Get job names
    const jobMap = new Map(jobs.map(j => [j.id, j.client_name]))

    // Generate CSV
    const exportId = `ATT-${Date.now()}-${Math.random().toString(36).substr(2, 9).toUpperCase()}`
    const now = new Date().toISOString()

    const { data: orgData } = await supabase
      .from('organizations')
      .select('name')
      .eq('id', organization_id)
      .single()

    const { data: userData } = await supabase
      .from('users')
      .select('full_name, role')
      .eq('id', userId)
      .single()

    const headerBlock = [
      'RiskMate Attestation Pack',
      `Export ID: ${exportId}`,
      `Generated: ${now}`,
      `Generated By: ${userData?.full_name || 'Unknown'} (${userData?.role || 'Unknown'})`,
      `Organization: ${orgData?.name || 'Unknown'}`,
      `Time Range: ${time_range}`,
      `Work Records: ${jobs.length}`,
      `Total Attestations: ${enrichedSignoffs.length}`,
      `Signed: ${enrichedSignoffs.filter(s => s.status === 'signed').length}`,
      `Pending: ${enrichedSignoffs.filter(s => s.status === 'pending').length}`,
      '',
      '--- Attestation Data ---',
    ]

    // Get audit log entries for these attestations to link back to ledger
    const signoffIds = enrichedSignoffs.map((s: any) => s.id)
    const { data: attestationLedgerEntries } = await supabase
      .from('audit_logs')
      .select('target_id, id as ledger_entry_id, created_at as ledger_entry_at')
      .in('target_id', signoffIds)
      .eq('target_type', 'signoff')
      .order('created_at', { ascending: false })
    
    const attestationLedgerMap = new Map<string, { ledger_entry_id: string; ledger_entry_at: string }>()
    attestationLedgerEntries?.forEach((entry: any) => {
      if (!attestationLedgerMap.has(entry.target_id)) {
        attestationLedgerMap.set(entry.target_id, { ledger_entry_id: entry.ledger_entry_id, ledger_entry_at: entry.ledger_entry_at })
      }
    })

    const headers = ['Attestation ID', 'Ledger Entry ID', 'Work Record ID', 'Work Record', 'Attestation Type', 'Signer ID', 'Signer', 'Role', 'Status at Export', 'Signed At (ISO)', 'IP Address', 'Comments']
    const rows = enrichedSignoffs.map((s: any) => {
      const ledgerInfo = attestationLedgerMap.get(s.id)
      return [
        s.id, // Stable primary key
        ledgerInfo?.ledger_entry_id || '', // Link to ledger entry
        s.job_id, // Work Record ID
        jobMap.get(s.job_id) || 'Unknown',
        s.signoff_type,
        s.signed_by || '', // User ID
        s.signer_name || 'Unknown',
        s.signer_role || 'Unknown',
        s.status, // Status at export time
        s.signed_at ? new Date(s.signed_at).toISOString() : '', // ISO format with timezone
        s.ip_address || '',
        s.comments || '',
      ]
    })

    const csv = [
      ...headerBlock,
      headers.join(','),
      ...rows.map((r: any[]) => r.map(cell => `"${String(cell).replace(/"/g, '""')}"`).join(',')),
    ].join('\n')

    res.setHeader('Content-Type', 'text/csv')
    res.setHeader('Content-Disposition', `attachment; filename="attestation-pack-${exportId}.csv"`)
    res.send(csv)

    // Log export event
    await supabase.from('audit_logs').insert({
      organization_id,
      actor_id: userId,
      event_name: 'audit.export',
      target_type: 'system',
      category: 'operations',
      outcome: 'allowed',
      severity: 'info',
      summary: `Exported Attestation Pack for ${jobs.length} work records`,
      metadata: { format: 'csv', export_type: 'attestations', export_id: exportId, filters: req.body },
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to export attestation pack',
      internalMessage: err?.message || String(err),
      code: 'AUDIT_EXPORT_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'AUDIT_EXPORT_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/export/attestations')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/export/pack
// Generates audit pack: bundles Ledger PDF + Controls CSV + Attestations CSV + Evidence Manifest
// Supports all saved view filters for deterministic proof pack generation
auditRouter.post('/export/pack', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId } = authReq.user
    const {
      time_range = '30d',
      job_id,
      site_id,
      category,
      actor_id,
      severity,
      outcome,
      view, // saved view preset
      start_date,
      end_date,
    } = req.body

    // Get organization and user info
    const { data: orgData } = await supabase
      .from('organizations')
      .select('name')
      .eq('id', organization_id)
      .single()

    const { data: userData } = await supabase
      .from('users')
      .select('full_name, role, email')
      .eq('id', userId)
      .single()

    // Generate deterministic pack ID from filters (same filters = same pack ID)
    // This enables reproducible proof packs for audit/verification
    const filterHash = crypto.createHash('sha256')
      .update(JSON.stringify({
        organization_id,
        time_range: time_range || '30d',
        job_id: job_id || null,
        site_id: site_id || null,
        category: category || null,
        actor_id: actor_id || null,
        severity: severity || null,
        outcome: outcome || null,
        view: view || null,
        start_date: start_date || null,
        end_date: end_date || null,
      }))
      .digest('hex')
      .substring(0, 16)
      .toUpperCase()
    
    const packId = `PACK-${filterHash}`

    // Create zip archive
    const archive = archiver('zip', { zlib: { level: 9 } })
    const chunks: Buffer[] = []

    archive.on('data', (chunk: Buffer) => chunks.push(chunk))
    archive.on('error', (err) => {
      throw err
    })

    // Variables for manifest
    let pdfBuffer: Buffer | null = null
    let events: any[] = []

    // Generate Ledger PDF
    try {
      const { generateLedgerExportPDF } = await import('../utils/pdf/ledgerExport')
      
      // Fetch events using unified filter logic (same as GET /events)
      let eventsQuery = supabase
        .from('audit_logs')
        .select('*')
        .order('created_at', { ascending: false })
        .limit(1000)

      // Apply unified filters for consistency (supports all saved view filters)
      try {
        eventsQuery = applyAuditFilters(eventsQuery, {
          organizationId: organization_id,
          ...(category && { category: category as any }),
          ...(job_id && { job_id: job_id }),
          ...(site_id && { site_id: site_id }),
          ...(actor_id && { actor_id: actor_id }),
          ...(severity && { severity: severity as any }),
          ...(outcome && { outcome: outcome as any }),
          ...(time_range && { time_range: time_range as any }),
          ...(start_date && { start_date: start_date }),
          ...(end_date && { end_date: end_date }),
          ...(view && { view: view as any }),
        })
      } catch (filterError: any) {
        if (filterError instanceof FilterValidationError) {
          const { response: errorResponse, errorId } = createErrorResponse({
            message: filterError.message,
            internalMessage: `Invalid filter parameter: ${filterError.field}`,
            code: 'INVALID_FILTER',
            requestId,
            statusCode: 400,
            field: filterError.field,
            allowedValues: filterError.allowedValues,
          })
          res.setHeader('X-Error-ID', errorId)
          logErrorForSupport(400, 'INVALID_FILTER', requestId, organization_id, errorResponse.message, errorResponse.internalMessage, 'system', 'info', '/api/audit/export/pack')
          return res.status(400).json(errorResponse)
        }
        throw filterError
      }

      const { data: eventsData, error: eventsError } = await eventsQuery

      if (eventsError) {
        throw new Error(`Database query failed: ${eventsError.message}`)
      }
      events = eventsData || []

      // Enrich events with actor and job info (simplified - full enrichment would match GET /events logic)
      const enrichedEvents = await Promise.all(
        events.map(async (e: any) => {
          const enriched: any = { ...e }
          if (e.actor_id) {
            const { data: actorData } = await supabase
              .from('users')
              .select('full_name, role')
              .eq('id', e.actor_id)
              .single()
            if (actorData) {
              enriched.actor_name = actorData.full_name || 'Unknown'
              enriched.actor_role = actorData.role || 'member'
            }
          }
          if (e.job_id) {
            const { data: jobData } = await supabase
              .from('jobs')
              .select('client_name')
              .eq('id', e.job_id)
              .single()
            if (jobData) {
              enriched.job_title = jobData.client_name
            }
          }
          return enriched
        })
      )

      const auditEntries = enrichedEvents.map((e: any) => ({
        id: e.id,
        event_name: e.event_name || e.event_type,
        created_at: e.created_at,
        category: e.category || 'operations',
        outcome: e.outcome || 'allowed',
        severity: e.severity || 'info',
        actor_name: e.actor_name || 'System',
        actor_role: e.actor_role || '',
        job_id: e.job_id,
        job_title: e.job_title,
        target_type: e.target_type,
        summary: e.summary,
      }))

      pdfBuffer = await generateLedgerExportPDF({
        organizationName: orgData?.name || 'Unknown',
        generatedBy: userData?.full_name || 'Unknown',
        generatedByRole: userData?.role || 'Unknown',
        exportId: packId,
        timeRange: time_range || 'All',
        filters: { job_id, site_id },
        events: auditEntries,
      })

      if (pdfBuffer) {
        archive.append(pdfBuffer, { name: `ledger_export_${packId}.pdf` })
      }
    } catch (err: any) {
      console.error('Failed to generate PDF for pack:', err)
      // Don't throw - continue with other files (CSV, manifest) even if PDF fails
      // This allows partial pack generation
    }

    // Generate Controls CSV (inline)
    let controlsBuffer: Buffer | null = null
    let controlsHash: string | null = null
    let controlsCount = 0
    try {
      const controlsResult = await generateControlsCSV(organization_id, userId, packId, job_id, site_id, time_range)
      controlsBuffer = controlsResult.buffer
      controlsCount = controlsResult.count
      controlsHash = crypto.createHash('sha256').update(controlsBuffer).digest('hex')
      archive.append(controlsBuffer, { name: `controls_${packId}.csv` })
    } catch (err: any) {
      console.error('Failed to generate Controls CSV for pack:', err)
      // Continue with other files
    }

    // Generate Attestations CSV (inline)
    let attestationsBuffer: Buffer | null = null
    let attestationsHash: string | null = null
    let attestationsCount = 0
    try {
      const attestationsResult = await generateAttestationsCSV(organization_id, userId, packId, job_id, site_id, time_range)
      attestationsBuffer = attestationsResult.buffer
      attestationsCount = attestationsResult.count
      attestationsHash = crypto.createHash('sha256').update(attestationsBuffer).digest('hex')
      archive.append(attestationsBuffer, { name: `attestations_${packId}.csv` })
    } catch (err: any) {
      console.error('Failed to generate Attestations CSV for pack:', err)
      // Continue with other files
    }

    // Calculate PDF hash
    const pdfHash = pdfBuffer ? crypto.createHash('sha256').update(pdfBuffer).digest('hex') : null

    // Generate manifest with counts and hashes (exact schema per specification)
    const manifest = {
      pack_id: packId,
      generated_at: new Date().toISOString(),
      generated_by: {
        user_id: userId,
        email: userData?.email || 'Unknown',
        role: userData?.role || 'Unknown',
      },
      filters: {
        time_range: time_range || '30d',
        start_date: start_date || null,
        end_date: end_date || null,
        job_id: job_id || null,
        site_id: site_id || null,
        category: category || null,
        actor_id: actor_id || null,
        severity: severity || null,
        outcome: outcome || null,
        view: view || null,
      },
      counts: {
        ledger_events: events.length,
        controls: controlsCount,
        attestations: attestationsCount,
      },
      files: [
        {
          name: `ledger_export_${packId}.pdf`,
          sha256: pdfHash || '',
          bytes: pdfBuffer?.length || 0,
        },
        {
          name: `controls_${packId}.csv`,
          sha256: controlsHash || '',
          bytes: controlsBuffer?.length || 0,
        },
        {
          name: `attestations_${packId}.csv`,
          sha256: attestationsHash || '',
          bytes: attestationsBuffer?.length || 0,
        },
        {
          name: `manifest_${packId}.json`,
          sha256: '', // Will be calculated after manifest is created
          bytes: 0, // Will be calculated
        },
      ],
    }

    // Calculate manifest hash and size
    const manifestJson = JSON.stringify(manifest, null, 2)
    const manifestBuffer = Buffer.from(manifestJson, 'utf-8')
    const manifestHash = crypto.createHash('sha256').update(manifestBuffer).digest('hex')
    manifest.files[3].sha256 = manifestHash
    manifest.files[3].bytes = manifestBuffer.length

    // Update archive with correct manifest
    archive.append(JSON.stringify(manifest, null, 2), { name: `manifest_${packId}.json` })

    await archive.finalize()

    // Wait for archive to finish
    await new Promise<void>((resolve, reject) => {
      archive.on('end', () => resolve())
      archive.on('error', reject)
    })

    const zipBuffer = Buffer.concat(chunks)

    res.setHeader('Content-Type', 'application/zip')
    res.setHeader('Content-Disposition', `attachment; filename="audit-pack-${packId}.zip"`)
    res.send(zipBuffer)

    // Store export pack metadata in ledger (immutable receipt per Compliance Ledger Contract)
    // Event: proof_pack.generated (per contract v1.0)
    await recordAuditLog({
      organizationId: organization_id,
      actorId: userId,
      eventName: 'proof_pack.generated',
      targetType: 'system',
      targetId: packId,
      metadata: {
        outcome: 'success',
        severity: 'info',
        summary: `Proof Pack generated (ID: ${packId}, ${events.length} events, ${controlsCount} controls, ${attestationsCount} attestations)`,
        format: 'zip',
        export_type: 'proof_pack',
        pack_id: packId,
        pack_type: 'evidence_slice', // Evidence Slice export per contract
        filters: {
          time_range: time_range || '30d',
          start_date: start_date || null,
          end_date: end_date || null,
          job_id: job_id || null,
          site_id: site_id || null,
          category: category || null,
          actor_id: actor_id || null,
          severity: severity || null,
          outcome: outcome || null,
          view: view || null,
        },
        file_hashes: {
          pdf: pdfHash,
          controls_csv: controlsHash,
          attestations_csv: attestationsHash,
          manifest: manifestHash,
        },
        counts: {
          ledger_events: events.length,
          controls: controlsCount,
          attestations: attestationsCount,
        },
        generated_at: new Date().toISOString(),
        generated_by: userData?.full_name || 'Unknown',
        generated_by_role: userData?.role || 'Unknown',
        generated_by_email: userData?.email || null,
        request_id: requestId,
        endpoint: '/api/audit/export/pack',
        summary: `Audit Pack exported (ID: ${packId})`,
      },
    })
  } catch (err: any) {
    // Determine error code and message based on error type
    let errorCode = 'AUDIT_EXPORT_ERROR'
    let userMessage = 'Failed to generate proof pack'
    let supportHint = 'Proof pack generation failed. Please try again or contact support with the error ID.'

    // Check for specific error types
    if (err?.code === 'ENOENT' || err?.message?.includes('Cannot find module')) {
      errorCode = 'BACKEND_CONFIG_ERROR'
      userMessage = 'Backend configuration error: Required module not found'
      supportHint = 'Backend services may be misconfigured. Contact support with error ID.'
    } else if (err?.message?.includes('timeout') || err?.message?.includes('ECONNREFUSED')) {
      errorCode = 'BACKEND_CONNECTION_ERROR'
      userMessage = 'Backend connection error: Services unreachable'
      supportHint = 'Backend services may be unavailable. Please try again in a moment.'
    } else if (err?.message?.includes('Database') || err?.message?.includes('query failed')) {
      errorCode = 'DATABASE_ERROR'
      userMessage = 'Database query failed during proof pack generation'
      supportHint = 'Database error occurred. Please try again or contact support with error ID.'
    } else if (err?.message?.includes('PDF') || err?.message?.includes('render')) {
      errorCode = 'PDF_GENERATION_ERROR'
      userMessage = 'PDF generation failed'
      supportHint = 'PDF rendering error. Please try again or contact support with error ID.'
    }

    const { response: errorResponse, errorId } = createErrorResponse({
      message: userMessage,
      internalMessage: err?.message || String(err),
      code: errorCode,
      requestId,
      statusCode: 500,
      supportHint,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, errorCode, requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/export/pack')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/assign
// Assigns an item (event, job, incident) to an owner with due date
// Ledger-first command: Validate → Mutate → Ledger Append (atomic)
auditRouter.post('/assign', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId, role: userRole = 'member', email: userEmail } = authReq.user
    const { target_type, target_id, owner_id, due_date, severity_override, note } = req.body
    const idempotencyKey = req.headers['idempotency-key'] as string

    // Validation
    if (!target_type || !target_id || !owner_id || !due_date) {
      return res.status(400).json({ 
        message: 'target_type, target_id, owner_id, and due_date are required' 
      })
    }

    // Authorization: Quick deny (executives cannot assign)
    if (userRole === 'executive') {
      // Still log the violation attempt
      await recordAuditLog({
        organizationId: organization_id,
        actorId: userId,
        eventName: 'auth.role_violation',
        targetType: target_type as any,
        targetId: target_id,
        metadata: {
          attempted_action: 'review.assigned',
          policy_statement: 'Executives have read-only access and cannot assign review items',
          endpoint: '/api/audit/assign',
        },
      })
      return res.status(403).json({ 
        code: 'AUTH_ROLE_READ_ONLY',
        message: 'Executives cannot assign review items' 
      })
    }

    // Build command context
    const ctx: CommandContext = {
      userId,
      organizationId: organization_id,
      userRole,
      userEmail: userEmail ?? undefined,
      requestId,
      endpoint: '/api/audit/assign',
      ip: req.ip || req.socket.remoteAddress || undefined,
      userAgent: req.headers['user-agent'] || undefined,
    }

    const options: CommandOptions = {
      idempotencyKey,
    }

    // Execute command: Validate → Mutate → Ledger Append (atomic)
    const result = await runCommand(
      supabase,
      ctx,
      options,
      async (tx) => {
        // Domain mutation: Verify target exists
        let targetExists = false
        let targetName: string | null = null

        if (target_type === 'job') {
          const { data } = await tx
            .from('jobs')
            .select('id, client_name')
            .eq('id', target_id)
            .eq('organization_id', organization_id)
            .single()
          targetExists = !!data
          targetName = data?.client_name || null
        } else if (target_type === 'event') {
          const { data } = await tx
            .from('audit_logs')
            .select('id, event_name')
            .eq('id', target_id)
            .eq('organization_id', organization_id)
            .single()
          targetExists = !!data
          targetName = data?.event_name || null
        }

        if (!targetExists) {
          throw new Error('Target not found')
        }

        // For now, assignment is stored in ledger metadata
        // In future, you might create an assignments table here
        return {
          target_type,
          target_id,
          target_name: targetName,
          owner_id,
          due_date,
          severity_override: severity_override || null,
          note: note || null,
          assigned_at: new Date().toISOString(),
        }
      },
      {
        eventName: 'review.assigned',
        targetType: target_type as any,
        targetId: target_id,
        metadata: {
          owner_id,
          due_date,
          severity_override: severity_override || null,
          note: note || null,
          assigned_at: new Date().toISOString(),
          summary: `Assigned to owner (due: ${due_date})`,
        },
      }
    )

    if (!result.ok) {
      const { response: errorResponse, errorId } = createErrorResponse({
        message: result.error?.message || 'Failed to assign item',
        internalMessage: result.error?.internalMessage,
        code: result.error?.code || 'ASSIGN_ERROR',
        requestId,
        statusCode: 500,
      })
      res.setHeader('X-Error-ID', errorId)
      logErrorForSupport(500, result.error?.code || 'ASSIGN_ERROR', requestId, organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/assign')
      return res.status(500).json(errorResponse)
    }

    res.json({ 
      success: true,
      message: 'Item assigned successfully',
      ledger_entry_id: result.ledger_entry_id,
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to assign item',
      internalMessage: err?.message || String(err),
      code: 'ASSIGN_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'ASSIGN_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/assign')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/resolve
// Resolves an item with reason, comment, and optional waiver
// Ledger-first command: Validate → Mutate → Ledger Append (atomic)
auditRouter.post('/resolve', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId, role: userRole = 'member', email: userEmail } = authReq.user
    const { target_type, target_id, reason, comment, requires_followup, waived, waiver_reason } = req.body
    const idempotencyKey = req.headers['idempotency-key'] as string

    // Validation
    if (!target_type || !target_id || !reason) {
      return res.status(400).json({ 
        message: 'target_type, target_id, and reason are required' 
      })
    }

    // Authorization: Quick deny (executives cannot resolve)
    if (userRole === 'executive') {
      await recordAuditLog({
        organizationId: organization_id,
        actorId: userId,
        eventName: 'auth.role_violation',
        targetType: target_type as any,
        targetId: target_id,
        metadata: {
          attempted_action: 'review.resolved',
          policy_statement: 'Executives have read-only access and cannot resolve review items',
          endpoint: '/api/audit/resolve',
        },
      })
      return res.status(403).json({ 
        code: 'AUTH_ROLE_READ_ONLY',
        message: 'Executives cannot resolve review items' 
      })
    }

    // Build command context
    const ctx: CommandContext = {
      userId,
      organizationId: organization_id,
      userRole,
      userEmail: userEmail ?? undefined,
      requestId,
      endpoint: '/api/audit/resolve',
      ip: req.ip || req.socket.remoteAddress || undefined,
      userAgent: req.headers['user-agent'] || undefined,
    }

    const options: CommandOptions = {
      idempotencyKey,
    }

    // Execute command: Validate → Mutate → Ledger Append (atomic)
    const eventName = waived ? 'review.waived' : 'review.resolved'
    const result = await runCommand(
      supabase,
      ctx,
      options,
      async (tx) => {
        // Domain mutation: Verify target exists
        let targetExists = false
        if (target_type === 'job') {
          const { data } = await tx
            .from('jobs')
            .select('id, client_name')
            .eq('id', target_id)
            .eq('organization_id', organization_id)
            .single()
          targetExists = !!data
        } else if (target_type === 'event') {
          const { data } = await tx
            .from('audit_logs')
            .select('id')
            .eq('id', target_id)
            .eq('organization_id', organization_id)
            .single()
          targetExists = !!data
        }

        if (!targetExists) {
          throw new Error('Target not found')
        }

        return {
          target_type,
          target_id,
          reason,
          comment: comment || null,
          requires_followup: requires_followup || false,
          waived: waived || false,
          waiver_reason: waived ? (waiver_reason || null) : null,
          resolved_at: new Date().toISOString(),
        }
      },
      {
        eventName,
        targetType: target_type as any,
        targetId: target_id,
        metadata: {
          reason,
          comment: comment || null,
          requires_followup: requires_followup || false,
          waived: waived || false,
          waiver_reason: waived ? (waiver_reason || null) : null,
          resolved_at: new Date().toISOString(),
          summary: waived 
            ? `Waived: ${reason}${waiver_reason ? ` - ${waiver_reason}` : ''}`
            : `Resolved: ${reason}${comment ? ` - ${comment}` : ''}`,
        },
      }
    )

    if (!result.ok) {
      const { response: errorResponse, errorId } = createErrorResponse({
        message: result.error?.message || 'Failed to resolve item',
        internalMessage: result.error?.internalMessage,
        code: result.error?.code || 'RESOLVE_ERROR',
        requestId,
        statusCode: 500,
      })
      res.setHeader('X-Error-ID', errorId)
      logErrorForSupport(500, result.error?.code || 'RESOLVE_ERROR', requestId, organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/resolve')
      return res.status(500).json(errorResponse)
    }

    res.json({ 
      success: true,
      message: waived ? 'Item waived successfully' : 'Item resolved successfully',
      ledger_entry_id: result.ledger_entry_id,
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to resolve item',
      internalMessage: err?.message || String(err),
      code: 'RESOLVE_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'RESOLVE_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/resolve')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/incidents/corrective-action
// Creates a corrective action (control) linked to an incident/work record
// Ledger-first command: Validate → Mutate → Ledger Append (atomic)
auditRouter.post('/incidents/corrective-action', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId, role: userRole = 'member', email: userEmail } = authReq.user
    const { work_record_id, incident_event_id, title, owner_id, due_date, verification_method, notes, severity } = req.body
    const idempotencyKey = req.headers['idempotency-key'] as string

    // Validation
    if (!work_record_id || !title || !owner_id || !due_date) {
      return res.status(400).json({ 
        message: 'work_record_id, title, owner_id, and due_date are required' 
      })
    }

    // Authorization: Quick deny (executives cannot create corrective actions)
    if (userRole === 'executive') {
      await recordAuditLog({
        organizationId: organization_id,
        actorId: userId,
        eventName: 'auth.role_violation',
        targetType: 'job',
        targetId: work_record_id,
        metadata: {
          attempted_action: 'incident.corrective_action.created',
          policy_statement: 'Executives have read-only access and cannot create corrective actions',
          endpoint: '/api/audit/incidents/corrective-action',
        },
      })
      return res.status(403).json({ 
        code: 'AUTH_ROLE_READ_ONLY',
        message: 'Executives cannot create corrective actions' 
      })
    }

    // Build command context
    const ctx: CommandContext = {
      userId,
      organizationId: organization_id,
      userRole,
      userEmail: userEmail ?? undefined,
      requestId,
      endpoint: '/api/audit/incidents/corrective-action',
      ip: req.ip || req.socket.remoteAddress || undefined,
      userAgent: req.headers['user-agent'] || undefined,
    }

    const options: CommandOptions = {
      idempotencyKey,
    }

    // Execute command: Validate → Mutate → Ledger Append (atomic)
    const result = await runCommand(
      supabase,
      ctx,
      options,
      async (tx) => {
        // Domain mutation: Verify work record exists
        const { data: job, error: jobError } = await tx
          .from('jobs')
          .select('id, client_name')
          .eq('id', work_record_id)
          .eq('organization_id', organization_id)
          .single()

        if (jobError || !job) {
          throw new Error('Work record not found')
        }

        // Create control (mitigation item)
        const { data: control, error: controlError } = await tx
          .from('mitigation_items')
          .insert({
            job_id: work_record_id,
            title,
            description: notes || `Corrective action for ${job.client_name}`,
            done: false,
            is_completed: false,
            owner_id,
            due_date,
            verification_method: verification_method || 'visual_inspection',
            severity: severity || 'info',
          })
          .select()
          .single()

        if (controlError || !control) {
          throw new Error('Failed to create corrective action')
        }

        return {
          control_id: control.id,
          work_record_id,
          incident_event_id: incident_event_id || null,
          title,
          owner_id,
          due_date,
          verification_method: verification_method || 'visual_inspection',
          notes: notes || null,
          severity: severity || 'info',
        }
      },
      {
        eventName: 'incident.corrective_action.created',
        targetType: 'mitigation',
        targetId: null, // Will be set from command result
        metadata: {
          work_record_id,
          incident_event_id: incident_event_id || null,
          title,
          owner_id,
          due_date,
          verification_method: verification_method || 'visual_inspection',
          notes: notes || null,
          severity: severity || 'info',
          summary: `Corrective action created: ${title}`,
        },
      }
    )

    if (!result.ok) {
      const { response: errorResponse, errorId } = createErrorResponse({
        message: result.error?.message || 'Failed to create corrective action',
        internalMessage: result.error?.internalMessage,
        code: result.error?.code || 'CORRECTIVE_ACTION_ERROR',
        requestId,
        statusCode: 500,
      })
      res.setHeader('X-Error-ID', errorId)
      logErrorForSupport(500, result.error?.code || 'CORRECTIVE_ACTION_ERROR', requestId, organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/incidents/corrective-action')
      return res.status(500).json(errorResponse)
    }

    // Update targetId with control_id from result
    const controlId = result.data?.control_id
    if (controlId) {
      // Update the ledger entry with the correct target_id
      await supabase
        .from('audit_logs')
        .update({ target_id: controlId })
        .eq('id', result.ledger_entry_id)
    }

    res.json({ 
      success: true,
      message: 'Corrective action created successfully',
      control_id: controlId,
      ledger_entry_id: result.ledger_entry_id,
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to create corrective action',
      internalMessage: err?.message || String(err),
      code: 'CORRECTIVE_ACTION_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'CORRECTIVE_ACTION_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/incidents/corrective-action')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/incidents/close
// Closes an incident with strict validation and creates attestation atomically
// Uses RPC function for true atomicity: domain mutations + ledger entries succeed/fail together
auditRouter.post('/incidents/close', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId, role: userRole = 'member' } = authReq.user
    const { 
      work_record_id, 
      closure_summary, 
      root_cause, 
      evidence_attached, 
      waived, 
      waiver_reason,
      no_action_required,
      no_action_justification,
    } = req.body
    const idempotencyKey = req.headers['idempotency-key'] as string

    // Validation
    if (!work_record_id || !closure_summary || !root_cause) {
      return res.status(400).json({ 
        message: 'work_record_id, closure_summary, and root_cause are required' 
      })
    }

    // Authorization: Quick deny (executives cannot close incidents)
    if (userRole === 'executive') {
      await recordAuditLog({
        organizationId: organization_id,
        actorId: userId,
        eventName: 'auth.role_violation',
        targetType: 'job',
        targetId: work_record_id,
        metadata: {
          attempted_action: 'incident.closed',
          policy_statement: 'Executives have read-only access and cannot close incidents',
          endpoint: '/api/audit/incidents/close',
        },
      })
      return res.status(403).json({ 
        code: 'AUTH_ROLE_READ_ONLY',
        message: 'Executives cannot close incidents' 
      })
    }

    // Use RPC function for atomic operation
    const { data: rpcResult, error: rpcError } = await supabase.rpc('audit_close_incident', {
      p_organization_id: organization_id,
      p_actor_id: userId,
      p_work_record_id: work_record_id,
      p_closure_summary: closure_summary,
      p_root_cause: root_cause,
      p_evidence_attached: evidence_attached || false,
      p_waived: waived || false,
      p_waiver_reason: waived ? waiver_reason : null,
      p_no_action_required: no_action_required || false,
      p_no_action_justification: no_action_required ? no_action_justification : null,
      p_request_id: requestId,
      p_endpoint: '/api/audit/incidents/close',
      p_ip: req.ip || req.socket.remoteAddress || null,
      p_user_agent: req.headers['user-agent'] || null,
      p_idempotency_key: idempotencyKey || null,
    })

    if (rpcError) {
      const errorMessage = rpcError.message || 'Failed to close incident'
      const { response: errorResponse, errorId } = createErrorResponse({
        message: errorMessage,
        internalMessage: rpcError.details || rpcError.message,
        code: 'CLOSE_INCIDENT_ERROR',
        requestId,
        statusCode: 500,
      })
      res.setHeader('X-Error-ID', errorId)
      logErrorForSupport(500, 'CLOSE_INCIDENT_ERROR', requestId, organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/incidents/close')
      return res.status(500).json(errorResponse)
    }

    if (!rpcResult?.ok) {
      return res.status(400).json({ 
        message: rpcResult?.error || 'Failed to close incident' 
      })
    }

    res.json({ 
      success: true,
      message: 'Incident closed successfully',
      attestation_id: rpcResult.attestation_id,
      ledger_entry_id: rpcResult.ledger_entry_id,
      attestation_ledger_entry_id: rpcResult.attestation_ledger_entry_id,
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to close incident',
      internalMessage: err?.message || String(err),
      code: 'CLOSE_INCIDENT_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'CLOSE_INCIDENT_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/incidents/close')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/access/revoke
// Revokes access for a user (disable, downgrade role, or revoke sessions)
// Uses RPC function for atomic operation
auditRouter.post('/access/revoke', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId, role: userRole = 'member' } = authReq.user
    const { target_user_id, action_type, reason, new_role } = req.body
    const idempotencyKey = req.headers['idempotency-key'] as string

    // Authorization: Only admin/owner can revoke
    if (userRole !== 'admin' && userRole !== 'owner') {
      await recordAuditLog({
        organizationId: organization_id,
        actorId: userId,
        eventName: 'auth.role_violation',
        targetType: 'user',
        targetId: target_user_id,
        metadata: {
          attempted_action: 'access.revoked',
          policy_statement: 'Only administrators and owners can revoke access',
          endpoint: '/api/audit/access/revoke',
        },
      })
      return res.status(403).json({ 
        code: 'AUTH_ROLE_FORBIDDEN',
        message: 'Only administrators and owners can revoke access' 
      })
    }

    // Validation
    if (!target_user_id || !action_type || !reason) {
      return res.status(400).json({ 
        message: 'target_user_id, action_type, and reason are required' 
      })
    }

    // Use RPC function for atomic operation
    const { data: rpcResult, error: rpcError } = await supabase.rpc('audit_revoke_access', {
      p_organization_id: organization_id,
      p_actor_id: userId,
      p_target_user_id: target_user_id,
      p_action_type: action_type,
      p_reason: reason,
      p_new_role: action_type === 'downgrade_role' ? new_role : null,
      p_request_id: requestId,
      p_endpoint: '/api/audit/access/revoke',
      p_ip: req.ip || req.socket.remoteAddress || null,
      p_user_agent: req.headers['user-agent'] || null,
      p_idempotency_key: idempotencyKey || null,
    })

    if (rpcError) {
      const errorMessage = rpcError.message || 'Failed to revoke access'
      const { response: errorResponse, errorId } = createErrorResponse({
        message: errorMessage,
        internalMessage: rpcError.details || rpcError.message,
        code: 'REVOKE_ACCESS_ERROR',
        requestId,
        statusCode: 500,
      })
      res.setHeader('X-Error-ID', errorId)
      logErrorForSupport(500, 'REVOKE_ACCESS_ERROR', requestId, organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/access/revoke')
      return res.status(500).json(errorResponse)
    }

    if (!rpcResult?.ok) {
      return res.status(400).json({ 
        message: rpcResult?.error || 'Failed to revoke access' 
      })
    }

    res.json({ 
      success: true,
      message: 'Access revoked successfully',
      ledger_entry_id: rpcResult.ledger_entry_id,
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to revoke access',
      internalMessage: err?.message || String(err),
      code: 'REVOKE_ACCESS_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'REVOKE_ACCESS_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/access/revoke')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/access/flag-suspicious
// Flags suspicious access activity and optionally opens a security incident
auditRouter.post('/access/flag-suspicious', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId } = authReq.user
    const { target_user_id, login_event_id, reason, notes, severity, open_incident } = req.body

    if (!target_user_id || !reason) {
      return res.status(400).json({ 
        message: 'target_user_id and reason are required' 
      })
    }

    if (reason === 'other' && !notes?.trim()) {
      return res.status(400).json({ 
        message: 'Notes are required when reason is "other"' 
      })
    }

    // Verify target user exists
    const { data: targetUser } = await supabase
      .from('users')
      .select('id, email, full_name')
      .eq('id', target_user_id)
      .eq('organization_id', organization_id)
      .single()

    if (!targetUser) {
      return res.status(404).json({ message: 'Target user not found' })
    }

    const idempotencyKey = req.headers['idempotency-key'] as string

    // Use RPC function for atomic operation
    const { data: rpcResult, error: rpcError } = await supabase.rpc('audit_flag_suspicious', {
      p_organization_id: organization_id,
      p_actor_id: userId,
      p_target_user_id: target_user_id,
      p_reason: reason,
      p_notes: notes || null,
      p_severity: severity || 'material',
      p_open_incident: open_incident !== false, // Default true
      p_login_event_id: login_event_id || null,
      p_request_id: requestId,
      p_endpoint: '/api/audit/access/flag-suspicious',
      p_ip: req.ip || req.socket.remoteAddress || null,
      p_user_agent: req.headers['user-agent'] || null,
      p_idempotency_key: idempotencyKey || null,
    })

    if (rpcError) {
      const errorMessage = rpcError.message || 'Failed to flag suspicious access'
      const { response: errorResponse, errorId } = createErrorResponse({
        message: errorMessage,
        internalMessage: rpcError.details || rpcError.message,
        code: 'FLAG_SUSPICIOUS_ERROR',
        requestId,
        statusCode: 500,
      })
      res.setHeader('X-Error-ID', errorId)
      logErrorForSupport(500, 'FLAG_SUSPICIOUS_ERROR', requestId, organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/access/flag-suspicious')
      return res.status(500).json(errorResponse)
    }

    if (!rpcResult?.ok) {
      return res.status(400).json({ 
        message: rpcResult?.error || 'Failed to flag suspicious access' 
      })
    }

    res.json({ 
      success: true,
      message: 'Suspicious access flagged successfully',
      ledger_entry_id: rpcResult.ledger_entry_id,
      incident_ledger_entry_id: rpcResult.incident_ledger_entry_id,
      incident_opened: !!rpcResult.incident_ledger_entry_id,
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to flag suspicious access',
      internalMessage: err?.message || String(err),
      code: 'FLAG_SUSPICIOUS_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'FLAG_SUSPICIOUS_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/access/flag-suspicious')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/readiness/resolve
// Unified endpoint for resolving readiness gaps (evidence/attestation/controls/etc.)
// Always emits readiness.resolved ledger event for audit trail
auditRouter.post('/readiness/resolve', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId, role: userRole = 'member', email: userEmail } = authReq.user
    const { readiness_item_id, rule_code, action_type, payload } = req.body
    const idempotencyKey = getIdempotencyKey(req) || undefined

    // Validation: Required fields
    if (!readiness_item_id || !rule_code || !action_type) {
      return res.status(400).json({ 
        code: 'VALIDATION_ERROR',
        message: 'readiness_item_id, rule_code, and action_type are required',
        fields: {
          readiness_item_id: !readiness_item_id ? 'required' : undefined,
          rule_code: !rule_code ? 'required' : undefined,
          action_type: !action_type ? 'required' : undefined,
        }
      })
    }

    // Validation: Action type and payload requirements
    const validActionTypes = ['create_evidence', 'request_attestation', 'create_control', 'mark_resolved']
    if (!validActionTypes.includes(action_type)) {
      return res.status(400).json({
        code: 'INVALID_ACTION_TYPE',
        message: `Invalid action_type: ${action_type}`,
        allowed_values: validActionTypes,
      })
    }

    // Validation: Payload requirements per action_type
    if (action_type === 'create_evidence') {
      const { job_id, name, file_path } = payload || {}
      if (!job_id || !name || !file_path) {
        return res.status(400).json({
          code: 'VALIDATION_ERROR',
          message: 'For create_evidence action_type: job_id, name, and file_path are required in payload',
          fields: {
            'payload.job_id': !job_id ? 'required' : undefined,
            'payload.name': !name ? 'required' : undefined,
            'payload.file_path': !file_path ? 'required' : undefined,
          }
        })
      }
      // Validate job belongs to organization
      const { data: job, error: jobError } = await supabase
        .from('jobs')
        .select('id, organization_id')
        .eq('id', job_id)
        .eq('organization_id', organization_id)
        .single()
      
      if (jobError || !job) {
        return res.status(404).json({
          code: 'JOB_NOT_FOUND',
          message: `Job ${job_id} not found or does not belong to your organization`,
        })
      }
    } else if (action_type === 'request_attestation') {
      const { job_id } = payload || {}
      if (!job_id) {
        return res.status(400).json({
          code: 'VALIDATION_ERROR',
          message: 'For request_attestation action_type: job_id is required in payload',
          fields: {
            'payload.job_id': 'required',
          }
        })
      }
      // Validate job belongs to organization
      const { data: job, error: jobError } = await supabase
        .from('jobs')
        .select('id, organization_id')
        .eq('id', job_id)
        .eq('organization_id', organization_id)
        .single()
      
      if (jobError || !job) {
        return res.status(404).json({
          code: 'JOB_NOT_FOUND',
          message: `Job ${job_id} not found or does not belong to your organization`,
        })
      }
    } else if (action_type === 'create_control') {
      const { job_id, title } = payload || {}
      if (!job_id || !title) {
        return res.status(400).json({
          code: 'VALIDATION_ERROR',
          message: 'For create_control action_type: job_id and title are required in payload',
          fields: {
            'payload.job_id': !job_id ? 'required' : undefined,
            'payload.title': !title ? 'required' : undefined,
          }
        })
      }
      // Validate job belongs to organization
      const { data: job, error: jobError } = await supabase
        .from('jobs')
        .select('id, organization_id')
        .eq('id', job_id)
        .eq('organization_id', organization_id)
        .single()
      
      if (jobError || !job) {
        return res.status(404).json({
          code: 'JOB_NOT_FOUND',
          message: `Job ${job_id} not found or does not belong to your organization`,
        })
      }
    }

    // Check idempotency if key provided
    if (idempotencyKey) {
      const idempotencyCheck = await checkIdempotency(
        supabase,
        idempotencyKey,
        organization_id,
        userId,
        '/api/audit/readiness/resolve'
      )

      if (idempotencyCheck?.isDuplicate && idempotencyCheck.response) {
        // Return cached response (replay)
        const cachedResponse = idempotencyCheck.response
        res.setHeader('X-Idempotency-Replayed', 'true')
        res.setHeader('X-Request-ID', requestId)
        res.status(cachedResponse.status)
        if (cachedResponse.headers) {
          Object.entries(cachedResponse.headers).forEach(([key, value]) => {
            res.setHeader(key, value)
          })
        }
        return res.json(cachedResponse.body)
      }
    }

    // Authorization: Quick deny (executives cannot resolve)
    if (userRole === 'executive') {
      await recordAuditLog({
        organizationId: organization_id,
        actorId: userId,
        eventName: 'auth.role_violation',
        targetType: 'system',
        targetId: readiness_item_id,
        metadata: {
          attempted_action: 'readiness.resolved',
          policy_statement: 'Executives have read-only access and cannot resolve readiness items',
          endpoint: '/api/audit/readiness/resolve',
        },
      })
      return res.status(403).json({ 
        code: 'AUTH_ROLE_READ_ONLY',
        message: 'Executives cannot resolve readiness items' 
      })
    }

    // Build command context
    const ctx: CommandContext = {
      userId,
      organizationId: organization_id,
      userRole,
      userEmail: userEmail ?? undefined,
      requestId,
      endpoint: '/api/audit/readiness/resolve',
      ip: req.ip || req.socket.remoteAddress || undefined,
      userAgent: req.headers['user-agent'] || undefined,
    }

    const options: CommandOptions = {
      idempotencyKey,
    }

    // Route to appropriate internal handler based on action_type
    let internalResult: any
    let targetId: string = readiness_item_id // Default fallback
    let targetType: string = 'system' // Default fallback
    let metadata: any = {
      readiness_item_id,
      rule_code,
      action_type,
      ...payload,
    }

    if (action_type === 'create_evidence') {
      // Create evidence/document (payload already validated above)
      const { job_id, name, file_path, file_size, mime_type, description } = payload || {}

      // Create document record
      const { data: document, error: docError } = await supabase
        .from('documents')
        .insert({
          job_id,
          organization_id,
          name,
          type: payload?.type || 'evidence',
          file_path,
          file_size: file_size || null,
          mime_type: mime_type || null,
          description: description || null,
          uploaded_by: userId,
        })
        .select()
        .single()

      if (docError) {
        throw new Error(`Failed to create evidence: ${docError.message}`)
      }

      internalResult = { document_id: document.id }
      targetId = document.id
      targetType = 'document'
      metadata.evidence_id = document.id
    } else if (action_type === 'request_attestation') {
      // Request attestation from user/role (job_id already validated above)
      const { job_id, target_user_id, target_role, due_date, message } = payload || {}

      // Create attestation request (stored in job_signoffs or similar)
      // For now, we'll create a ledger entry and notification
      // In a full implementation, this would create a signoff record
      targetId = job_id
      targetType = 'job'
      metadata.target_user_id = target_user_id
      metadata.target_role = target_role
      metadata.due_date = due_date
      metadata.message = message

      internalResult = { 
        attestation_requested: true,
        target_user_id,
        target_role,
        due_date,
      }
    } else if (action_type === 'create_control') {
      // Create control/mitigation item (job_id and title already validated above)
      const { job_id, title, description, owner_id, due_date } = payload || {}

      const { data: control, error: controlError } = await supabase
        .from('mitigation_items')
        .insert({
          job_id,
          title,
          description: description || null,
          owner_id: owner_id || null,
          due_date: due_date || null,
          done: false,
          is_completed: false,
        })
        .select()
        .single()

      if (controlError) {
        throw new Error(`Failed to create control: ${controlError.message}`)
      }

      internalResult = { control_id: control.id }
      targetId = control.id
      targetType = 'mitigation'
      metadata.control_id = control.id
    } else if (action_type === 'mark_resolved') {
      // Simply mark as resolved without creating anything (no payload validation needed)
      targetId = readiness_item_id
      targetType = 'system'
      internalResult = { resolved: true }
    }

    // Execute command: Emit readiness.resolved ledger event
    const result = await runCommand(
      supabase,
      ctx,
      options,
      async (tx) => {
        // Domain mutations already done above, just return result
        return internalResult
      },
      {
        eventName: 'readiness.resolved',
        targetType: targetType as any,
        targetId: targetId,
        metadata: {
          ...metadata,
          summary: `Resolved readiness gap: ${rule_code} via ${action_type}`,
        },
      }
    )

    if (!result.ok) {
      const { response: errorResponse, errorId } = createErrorResponse({
        message: result.error?.message || 'Failed to resolve readiness item',
        internalMessage: result.error?.internalMessage,
        code: result.error?.code || 'READINESS_RESOLVE_ERROR',
        requestId,
        statusCode: 500,
      })
      res.setHeader('X-Error-ID', errorId)
      logErrorForSupport(500, result.error?.code || 'READINESS_RESOLVE_ERROR', requestId, organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/readiness/resolve')
      return res.status(500).json(errorResponse)
    }

    const successResponse = {
      success: true,
      message: 'Readiness item resolved successfully',
      ledger_entry_id: result.ledger_entry_id,
      action_type,
      result: internalResult,
    }

    // Set response headers
    res.setHeader('X-Idempotency-Replayed', 'false')
    res.setHeader('X-Request-ID', requestId)

    // Store idempotency key if provided (for future replay detection)
    if (idempotencyKey) {
      await storeIdempotencyKey(
        supabase,
        idempotencyKey,
        organization_id,
        userId,
        '/api/audit/readiness/resolve',
        200,
        successResponse,
        { 'X-Request-ID': requestId, 'X-Idempotency-Replayed': 'false' }
      )
    }

    res.json(successResponse)
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to resolve readiness item',
      internalMessage: err?.message || String(err),
      code: 'READINESS_RESOLVE_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'READINESS_RESOLVE_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/readiness/resolve')
    res.status(500).json(errorResponse)
  }
})

// POST /api/audit/readiness/bulk-resolve
// Bulk resolve multiple readiness items with partial failure handling
auditRouter.post('/readiness/bulk-resolve', authenticate as unknown as express.RequestHandler, async (req: express.Request, res: express.Response) => {
  const authReq = req as AuthenticatedRequest & RequestWithId
  const requestId = authReq.requestId || 'unknown'
  
  try {
    const { organization_id, id: userId, role: userRole = 'member' } = authReq.user
    const { items } = req.body // Array of { readiness_item_id, rule_code, action_type, payload }
    const idempotencyKey = getIdempotencyKey(req) || undefined

    // Validation
    if (!Array.isArray(items) || items.length === 0) {
      return res.status(400).json({ 
        code: 'VALIDATION_ERROR',
        message: 'items array is required and must not be empty' 
      })
    }

    // Hard limit: prevent abuse
    if (items.length > 100) {
      return res.status(400).json({ 
        code: 'BULK_LIMIT_EXCEEDED',
        message: 'Maximum 100 items allowed per bulk resolve request',
        provided: items.length,
        maximum: 100,
      })
    }

    // Authorization: Quick deny (executives cannot resolve)
    if (userRole === 'executive') {
      await recordAuditLog({
        organizationId: organization_id,
        actorId: userId,
        eventName: 'auth.role_violation',
        targetType: 'system',
        targetId: 'bulk',
        metadata: {
          attempted_action: 'readiness.bulk_resolved',
          policy_statement: 'Executives have read-only access and cannot resolve readiness items',
          endpoint: '/api/audit/readiness/bulk-resolve',
        },
      })
      return res.status(403).json({ 
        code: 'AUTH_ROLE_READ_ONLY',
        message: 'Executives cannot resolve readiness items' 
      })
    }

    // Process items in parallel (with concurrency limit)
    const results: Array<{ item: any; success: boolean; result?: any; error?: string }> = []
    const BATCH_SIZE = 10 // Process 10 at a time to avoid overwhelming the system

    for (let i = 0; i < items.length; i += BATCH_SIZE) {
      const batch = items.slice(i, i + BATCH_SIZE)
      const batchResults = await Promise.all(
        batch.map(async (item: any) => {
          try {
            // Create internal request to resolve endpoint
            // We'll call the resolve logic directly for efficiency
            const resolvePayload = {
              readiness_item_id: item.readiness_item_id,
              rule_code: item.rule_code,
              action_type: item.action_type,
              payload: item.payload,
            }

            // For bulk operations, generate unique idempotency key per item if not provided
            const itemIdempotencyKey = item.idempotency_key || `${idempotencyKey || crypto.randomUUID()}-${item.readiness_item_id}`

            // Check idempotency for this specific item
            const idempotencyCheck = await checkIdempotency(
              supabase,
              itemIdempotencyKey,
              organization_id,
              userId,
              '/api/audit/readiness/resolve'
            )

            if (idempotencyCheck?.isDuplicate && idempotencyCheck.response) {
              return {
                item,
                success: true,
                result: idempotencyCheck.response.body,
                fromCache: true,
              }
            }

            // Process the resolve (simplified - in production, extract resolve logic to shared function)
            // For now, we'll create a simplified version that handles the common cases
            const { readiness_item_id, rule_code, action_type, payload: itemPayload } = resolvePayload

            if (action_type === 'create_evidence') {
              const { job_id, name, file_path } = itemPayload || {}
              if (!job_id || !name || !file_path) {
                throw new Error('job_id, name, and file_path required for create_evidence')
              }

              const { data: document, error: docError } = await supabase
                .from('documents')
                .insert({
                  job_id,
                  organization_id,
                  name,
                  type: itemPayload?.type || 'evidence',
                  file_path,
                  file_size: itemPayload?.file_size || null,
                  mime_type: itemPayload?.mime_type || null,
                  description: itemPayload?.description || null,
                  uploaded_by: userId,
                })
                .select()
                .single()

              if (docError) throw new Error(`Failed to create evidence: ${docError.message}`)

              // Emit readiness.resolved ledger event
              await recordAuditLog({
                organizationId: organization_id,
                actorId: userId,
                eventName: 'readiness.resolved',
                targetType: 'document',
                targetId: document.id,
                metadata: {
                  readiness_item_id,
                  rule_code,
                  action_type,
                  evidence_id: document.id,
                  summary: `Resolved readiness gap: ${rule_code} via ${action_type}`,
                },
              })

              return {
                item,
                success: true,
                result: { document_id: document.id },
              }
            } else if (action_type === 'mark_resolved') {
              // Emit readiness.resolved ledger event
              await recordAuditLog({
                organizationId: organization_id,
                actorId: userId,
                eventName: 'readiness.resolved',
                targetType: 'system',
                targetId: readiness_item_id,
                metadata: {
                  readiness_item_id,
                  rule_code,
                  action_type,
                  summary: `Resolved readiness gap: ${rule_code} via ${action_type}`,
                },
              })

              // Store idempotency key for future duplicate detection
              if (itemIdempotencyKey) {
                await storeIdempotencyKey(
                  supabase,
                  itemIdempotencyKey,
                  organization_id,
                  userId,
                  '/api/audit/readiness/resolve',
                  200,
                  { success: true },
                  {}
                )
              }

              return {
                item,
                success: true,
                result: { resolved: true },
              }
            } else {
              // For other action types, return error (they can be handled individually via single resolve endpoint)
              throw new Error(`Bulk resolve not yet supported for action_type: ${action_type}`)
            }
          } catch (error: any) {
            return {
              item,
              success: false,
              error: error.message || String(error),
            }
          }
        })
      )

      results.push(...batchResults)
    }

    // Count successes and failures
    const successful = results.filter(r => r.success)
    const failed = results.filter(r => !r.success)

    // Log bulk operation to ledger
    await recordAuditLog({
      organizationId: organization_id,
      actorId: userId,
      eventName: 'readiness.bulk_resolved',
      targetType: 'system',
      targetId: 'bulk',
      metadata: {
        total_items: items.length,
        successful_count: successful.length,
        failed_count: failed.length,
        items: results.map(r => ({
          readiness_item_id: r.item?.readiness_item_id,
          rule_code: r.item?.rule_code,
          success: r.success,
          error: r.error,
        })),
        summary: `Bulk resolved ${successful.length}/${items.length} readiness items`,
      },
    })

    // Set response headers
    res.setHeader('X-Idempotency-Replayed', 'false')
    res.setHeader('X-Request-ID', requestId)

    res.json({
      success: true,
      total: items.length,
      successful: successful.length,
      failed: failed.length,
      results: results.map(r => ({
        readiness_item_id: r.item?.readiness_item_id,
        rule_code: r.item?.rule_code,
        success: r.success,
        result: r.result,
        error: r.error,
      })),
      // Return failed items for UI to keep selected
      failed_items: failed.map(r => ({
        readiness_item_id: r.item?.readiness_item_id,
        rule_code: r.item?.rule_code,
        action_type: r.item?.action_type,
        error: r.error,
      })),
    })
  } catch (err: any) {
    const { response: errorResponse, errorId } = createErrorResponse({
      message: 'Failed to bulk resolve readiness items',
      internalMessage: err?.message || String(err),
      code: 'READINESS_BULK_RESOLVE_ERROR',
      requestId,
      statusCode: 500,
    })
    res.setHeader('X-Error-ID', errorId)
    logErrorForSupport(500, 'READINESS_BULK_RESOLVE_ERROR', requestId, authReq.user?.organization_id, errorResponse.message, errorResponse.internal_message, errorResponse.category, errorResponse.severity, '/api/audit/readiness/bulk-resolve')
    res.status(500).json(errorResponse)
  }
})
